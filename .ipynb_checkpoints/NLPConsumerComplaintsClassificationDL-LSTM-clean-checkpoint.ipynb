{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4870212e-8584-458a-8b68-6b7567dffd9d",
   "metadata": {},
   "source": [
    "# Install requirements"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "69b4a324",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting tensorflow_addons\n",
      "  Downloading tensorflow_addons-0.14.0-cp39-cp39-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (1.1 MB)\n",
      "\u001b[K     |████████████████████████████████| 1.1 MB 31.7 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: numpy in /opt/app-root/lib/python3.9/site-packages (1.21.0)\n",
      "Requirement already satisfied: pandas in /opt/app-root/lib/python3.9/site-packages (1.2.5)\n",
      "Collecting tensorflow\n",
      "  Downloading tensorflow-2.6.0-cp39-cp39-manylinux2010_x86_64.whl (458.4 MB)\n",
      "\u001b[K     |████████████████████████████████| 458.4 MB 175.6 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting sklearn\n",
      "  Downloading sklearn-0.0.tar.gz (1.1 kB)\n",
      "Collecting nltk\n",
      "  Downloading nltk-3.6.5-py3-none-any.whl (1.5 MB)\n",
      "\u001b[K     |████████████████████████████████| 1.5 MB 99.7 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting spacy\n",
      "  Downloading spacy-3.1.3-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (5.9 MB)\n",
      "\u001b[K     |████████████████████████████████| 5.9 MB 45.0 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting textblob\n",
      "  Downloading textblob-0.15.3-py2.py3-none-any.whl (636 kB)\n",
      "\u001b[K     |████████████████████████████████| 636 kB 109.5 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting gensim\n",
      "  Downloading gensim-4.1.2-cp39-cp39-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (24.0 MB)\n",
      "\u001b[K     |████████████████████████████████| 24.0 MB 39.4 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: scipy in /opt/app-root/lib/python3.9/site-packages (1.7.0)\n",
      "Requirement already satisfied: seaborn in /opt/app-root/lib/python3.9/site-packages (0.11.1)\n",
      "Requirement already satisfied: matplotlib in /opt/app-root/lib/python3.9/site-packages (3.4.2)\n",
      "Requirement already satisfied: minio in /opt/app-root/lib/python3.9/site-packages (6.0.2)\n",
      "Collecting mlflow\n",
      "  Downloading mlflow-1.20.2-py3-none-any.whl (14.6 MB)\n",
      "\u001b[K     |████████████████████████████████| 14.6 MB 103.1 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting wordcloud\n",
      "  Downloading wordcloud-1.8.1-cp39-cp39-manylinux1_x86_64.whl (363 kB)\n",
      "\u001b[K     |████████████████████████████████| 363 kB 102.1 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: boto3 in /opt/app-root/lib/python3.9/site-packages (1.17.11)\n",
      "Collecting typeguard>=2.7\n",
      "  Downloading typeguard-2.13.0-py3-none-any.whl (17 kB)\n",
      "Requirement already satisfied: python-dateutil>=2.7.3 in /opt/app-root/lib/python3.9/site-packages (from pandas) (2.8.1)\n",
      "Requirement already satisfied: pytz>=2017.3 in /opt/app-root/lib/python3.9/site-packages (from pandas) (2021.1)\n",
      "Collecting gast==0.4.0\n",
      "  Downloading gast-0.4.0-py3-none-any.whl (9.8 kB)\n",
      "Collecting clang~=5.0\n",
      "  Downloading clang-5.0.tar.gz (30 kB)\n",
      "Collecting google-pasta~=0.2\n",
      "  Downloading google_pasta-0.2.0-py3-none-any.whl (57 kB)\n",
      "\u001b[K     |████████████████████████████████| 57 kB 81.6 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting six~=1.15.0\n",
      "  Downloading six-1.15.0-py2.py3-none-any.whl (10 kB)\n",
      "Collecting flatbuffers~=1.12.0\n",
      "  Downloading flatbuffers-1.12-py2.py3-none-any.whl (15 kB)\n",
      "Requirement already satisfied: absl-py~=0.10 in /opt/app-root/lib/python3.9/site-packages (from tensorflow) (0.11.0)\n",
      "Requirement already satisfied: wheel~=0.35 in /opt/app-root/lib/python3.9/site-packages (from tensorflow) (0.36.2)\n",
      "Requirement already satisfied: wrapt~=1.12.1 in /opt/app-root/lib/python3.9/site-packages (from tensorflow) (1.12.1)\n",
      "Collecting h5py~=3.1.0\n",
      "  Downloading h5py-3.1.0-cp39-cp39-manylinux1_x86_64.whl (4.4 MB)\n",
      "\u001b[K     |████████████████████████████████| 4.4 MB 118.5 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting keras-preprocessing~=1.1.2\n",
      "  Downloading Keras_Preprocessing-1.1.2-py2.py3-none-any.whl (42 kB)\n",
      "\u001b[K     |████████████████████████████████| 42 kB 55.9 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting grpcio<2.0,>=1.37.0\n",
      "  Downloading grpcio-1.41.0-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.9 MB)\n",
      "\u001b[K     |████████████████████████████████| 3.9 MB 95.5 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting opt-einsum~=3.3.0\n",
      "  Downloading opt_einsum-3.3.0-py3-none-any.whl (65 kB)\n",
      "\u001b[K     |████████████████████████████████| 65 kB 95.5 MB/s  eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: protobuf>=3.9.2 in /opt/app-root/lib/python3.9/site-packages (from tensorflow) (3.17.3)\n",
      "Collecting tensorboard~=2.6\n",
      "  Downloading tensorboard-2.7.0-py3-none-any.whl (5.8 MB)\n",
      "\u001b[K     |████████████████████████████████| 5.8 MB 119.3 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting keras~=2.6\n",
      "  Downloading keras-2.6.0-py2.py3-none-any.whl (1.3 MB)\n",
      "\u001b[K     |████████████████████████████████| 1.3 MB 42.0 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting typing-extensions~=3.7.4\n",
      "  Downloading typing_extensions-3.7.4.3-py3-none-any.whl (22 kB)\n",
      "Collecting astunparse~=1.6.3\n",
      "  Downloading astunparse-1.6.3-py2.py3-none-any.whl (12 kB)\n",
      "Requirement already satisfied: termcolor~=1.1.0 in /opt/app-root/lib/python3.9/site-packages (from tensorflow) (1.1.0)\n",
      "Collecting tensorflow-estimator~=2.6\n",
      "  Downloading tensorflow_estimator-2.6.0-py2.py3-none-any.whl (462 kB)\n",
      "\u001b[K     |████████████████████████████████| 462 kB 91.6 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: scikit-learn in /opt/app-root/lib/python3.9/site-packages (from sklearn) (0.24.2)\n",
      "Requirement already satisfied: joblib in /opt/app-root/lib/python3.9/site-packages (from nltk) (1.0.1)\n",
      "Requirement already satisfied: click in /opt/app-root/lib/python3.9/site-packages (from nltk) (7.1.2)\n",
      "Requirement already satisfied: tqdm in /opt/app-root/lib/python3.9/site-packages (from nltk) (4.61.1)\n",
      "Collecting regex>=2021.8.3\n",
      "  Downloading regex-2021.10.21-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (761 kB)\n",
      "\u001b[K     |████████████████████████████████| 761 kB 83.4 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting thinc<8.1.0,>=8.0.9\n",
      "  Downloading thinc-8.0.11-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (635 kB)\n",
      "\u001b[K     |████████████████████████████████| 635 kB 101.1 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting cymem<2.1.0,>=2.0.2\n",
      "  Downloading cymem-2.0.5-cp39-cp39-manylinux2014_x86_64.whl (34 kB)\n",
      "Requirement already satisfied: packaging>=20.0 in /opt/app-root/lib/python3.9/site-packages (from spacy) (20.9)\n",
      "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /opt/app-root/lib/python3.9/site-packages (from spacy) (2.25.1)\n",
      "Collecting preshed<3.1.0,>=3.0.2\n",
      "  Downloading preshed-3.0.5-cp39-cp39-manylinux2014_x86_64.whl (127 kB)\n",
      "\u001b[K     |████████████████████████████████| 127 kB 118.1 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: pydantic!=1.8,!=1.8.1,<1.9.0,>=1.7.4 in /opt/app-root/lib/python3.9/site-packages (from spacy) (1.8.2)\n",
      "Collecting catalogue<2.1.0,>=2.0.6\n",
      "  Downloading catalogue-2.0.6-py3-none-any.whl (17 kB)\n",
      "Requirement already satisfied: typer<0.5.0,>=0.3.0 in /opt/app-root/lib/python3.9/site-packages (from spacy) (0.4.0)\n",
      "Requirement already satisfied: jinja2 in /opt/app-root/lib/python3.9/site-packages (from spacy) (2.11.3)\n",
      "Requirement already satisfied: setuptools in /opt/app-root/lib/python3.9/site-packages (from spacy) (58.0.4)\n",
      "Collecting pathy>=0.3.5\n",
      "  Downloading pathy-0.6.0-py3-none-any.whl (42 kB)\n",
      "\u001b[K     |████████████████████████████████| 42 kB 57.8 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting blis<0.8.0,>=0.4.0\n",
      "  Downloading blis-0.7.4-cp39-cp39-manylinux2014_x86_64.whl (9.8 MB)\n",
      "\u001b[K     |████████████████████████████████| 9.8 MB 39.4 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting wasabi<1.1.0,>=0.8.1\n",
      "  Downloading wasabi-0.8.2-py3-none-any.whl (23 kB)\n",
      "Collecting murmurhash<1.1.0,>=0.28.0\n",
      "  Downloading murmurhash-1.0.5-cp39-cp39-manylinux2014_x86_64.whl (20 kB)\n",
      "Collecting spacy-legacy<3.1.0,>=3.0.8\n",
      "  Downloading spacy_legacy-3.0.8-py2.py3-none-any.whl (14 kB)\n",
      "Collecting srsly<3.0.0,>=2.4.1\n",
      "  Downloading srsly-2.4.1-cp39-cp39-manylinux2014_x86_64.whl (457 kB)\n",
      "\u001b[K     |████████████████████████████████| 457 kB 113.0 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting smart-open>=1.8.1\n",
      "  Downloading smart_open-5.2.1-py3-none-any.whl (58 kB)\n",
      "\u001b[K     |████████████████████████████████| 58 kB 89.2 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: kiwisolver>=1.0.1 in /opt/app-root/lib/python3.9/site-packages (from matplotlib) (1.3.1)\n",
      "Requirement already satisfied: pillow>=6.2.0 in /opt/app-root/lib/python3.9/site-packages (from matplotlib) (8.2.0)\n",
      "Requirement already satisfied: pyparsing>=2.2.1 in /opt/app-root/lib/python3.9/site-packages (from matplotlib) (2.4.7)\n",
      "Requirement already satisfied: cycler>=0.10 in /opt/app-root/lib/python3.9/site-packages (from matplotlib) (0.10.0)\n",
      "Requirement already satisfied: configparser in /opt/app-root/lib/python3.9/site-packages (from minio) (5.0.2)\n",
      "Requirement already satisfied: urllib3 in /opt/app-root/lib/python3.9/site-packages (from minio) (1.26.6)\n",
      "Requirement already satisfied: certifi in /opt/app-root/lib/python3.9/site-packages (from minio) (2021.5.30)\n",
      "Requirement already satisfied: gitpython>=2.1.0 in /opt/app-root/lib/python3.9/site-packages (from mlflow) (3.1.18)\n",
      "Collecting databricks-cli>=0.8.7\n",
      "  Downloading databricks-cli-0.16.2.tar.gz (58 kB)\n",
      "\u001b[K     |████████████████████████████████| 58 kB 94.4 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting gunicorn; platform_system != \"Windows\"\n",
      "  Downloading gunicorn-20.1.0-py3-none-any.whl (79 kB)\n",
      "\u001b[K     |████████████████████████████████| 79 kB 97.3 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: cloudpickle in /opt/app-root/lib/python3.9/site-packages (from mlflow) (2.0.0)\n",
      "Requirement already satisfied: entrypoints in /opt/app-root/lib/python3.9/site-packages (from mlflow) (0.3)\n",
      "Requirement already satisfied: importlib-metadata!=4.7.0,>=3.7.0 in /opt/app-root/lib/python3.9/site-packages (from mlflow) (4.6.0)\n",
      "Collecting prometheus-flask-exporter\n",
      "  Downloading prometheus_flask_exporter-0.18.3-py3-none-any.whl (17 kB)\n",
      "Collecting docker>=4.0.0\n",
      "  Downloading docker-5.0.3-py2.py3-none-any.whl (146 kB)\n",
      "\u001b[K     |████████████████████████████████| 146 kB 108.9 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting sqlparse>=0.3.1\n",
      "  Downloading sqlparse-0.4.2-py3-none-any.whl (42 kB)\n",
      "\u001b[K     |████████████████████████████████| 42 kB 51.6 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting querystring-parser\n",
      "  Downloading querystring_parser-1.2.4-py2.py3-none-any.whl (7.9 kB)\n",
      "Collecting Flask\n",
      "  Downloading Flask-2.0.2-py3-none-any.whl (95 kB)\n",
      "\u001b[K     |████████████████████████████████| 95 kB 99.8 MB/s  eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: sqlalchemy in /opt/app-root/lib/python3.9/site-packages (from mlflow) (1.4.19)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /opt/app-root/lib/python3.9/site-packages (from mlflow) (5.4.1)\n",
      "Collecting alembic<=1.4.1\n",
      "  Downloading alembic-1.4.1.tar.gz (1.1 MB)\n",
      "\u001b[K     |████████████████████████████████| 1.1 MB 113.3 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: s3transfer<0.4.0,>=0.3.0 in /opt/app-root/lib/python3.9/site-packages (from boto3) (0.3.7)\n",
      "Requirement already satisfied: jmespath<1.0.0,>=0.7.1 in /opt/app-root/lib/python3.9/site-packages (from boto3) (0.10.0)\n",
      "Requirement already satisfied: botocore<1.21.0,>=1.20.11 in /opt/app-root/lib/python3.9/site-packages (from boto3) (1.20.101)\n",
      "Requirement already satisfied: google-auth<3,>=1.6.3 in /opt/app-root/lib/python3.9/site-packages (from tensorboard~=2.6->tensorflow) (1.32.0)\n",
      "Collecting google-auth-oauthlib<0.5,>=0.4.1\n",
      "  Downloading google_auth_oauthlib-0.4.6-py2.py3-none-any.whl (18 kB)\n",
      "Collecting tensorboard-data-server<0.7.0,>=0.6.0\n",
      "  Downloading tensorboard_data_server-0.6.1-py3-none-manylinux2010_x86_64.whl (4.9 MB)\n",
      "\u001b[K     |████████████████████████████████| 4.9 MB 114.9 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting tensorboard-plugin-wit>=1.6.0\n",
      "  Downloading tensorboard_plugin_wit-1.8.0-py3-none-any.whl (781 kB)\n",
      "\u001b[K     |████████████████████████████████| 781 kB 110.6 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting markdown>=2.6.8\n",
      "  Downloading Markdown-3.3.4-py3-none-any.whl (97 kB)\n",
      "\u001b[K     |████████████████████████████████| 97 kB 109.5 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting werkzeug>=0.11.15\n",
      "  Downloading Werkzeug-2.0.2-py3-none-any.whl (288 kB)\n",
      "\u001b[K     |████████████████████████████████| 288 kB 118.3 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: threadpoolctl>=2.0.0 in /opt/app-root/lib/python3.9/site-packages (from scikit-learn->sklearn) (2.1.0)\n",
      "Requirement already satisfied: idna<3,>=2.5 in /opt/app-root/lib/python3.9/site-packages (from requests<3.0.0,>=2.13.0->spacy) (2.10)\n",
      "Requirement already satisfied: chardet<5,>=3.0.2 in /opt/app-root/lib/python3.9/site-packages (from requests<3.0.0,>=2.13.0->spacy) (4.0.0)\n",
      "Requirement already satisfied: MarkupSafe>=0.23 in /opt/app-root/lib/python3.9/site-packages (from jinja2->spacy) (2.0.1)\n",
      "Requirement already satisfied: gitdb<5,>=4.0.1 in /opt/app-root/lib/python3.9/site-packages (from gitpython>=2.1.0->mlflow) (4.0.7)\n",
      "Requirement already satisfied: tabulate>=0.7.7 in /opt/app-root/lib/python3.9/site-packages (from databricks-cli>=0.8.7->mlflow) (0.8.9)\n",
      "Requirement already satisfied: zipp>=0.5 in /opt/app-root/lib/python3.9/site-packages (from importlib-metadata!=4.7.0,>=3.7.0->mlflow) (3.4.1)\n",
      "Requirement already satisfied: prometheus-client in /opt/app-root/lib/python3.9/site-packages (from prometheus-flask-exporter->mlflow) (0.11.0)\n",
      "Requirement already satisfied: websocket-client>=0.32.0 in /opt/app-root/lib/python3.9/site-packages (from docker>=4.0.0->mlflow) (1.1.0)\n",
      "Collecting itsdangerous>=2.0\n",
      "  Downloading itsdangerous-2.0.1-py3-none-any.whl (18 kB)\n",
      "Requirement already satisfied: greenlet!=0.4.17; python_version >= \"3\" in /opt/app-root/lib/python3.9/site-packages (from sqlalchemy->mlflow) (1.1.0)\n",
      "Requirement already satisfied: Mako in /opt/app-root/lib/python3.9/site-packages (from alembic<=1.4.1->mlflow) (1.1.5)\n",
      "Collecting python-editor>=0.3\n",
      "  Downloading python_editor-1.0.4-py3-none-any.whl (4.9 kB)\n",
      "Requirement already satisfied: cachetools<5.0,>=2.0.0 in /opt/app-root/lib/python3.9/site-packages (from google-auth<3,>=1.6.3->tensorboard~=2.6->tensorflow) (4.2.2)\n",
      "Requirement already satisfied: pyasn1-modules>=0.2.1 in /opt/app-root/lib/python3.9/site-packages (from google-auth<3,>=1.6.3->tensorboard~=2.6->tensorflow) (0.2.8)\n",
      "Requirement already satisfied: rsa<5,>=3.1.4; python_version >= \"3.6\" in /opt/app-root/lib/python3.9/site-packages (from google-auth<3,>=1.6.3->tensorboard~=2.6->tensorflow) (4.7.2)\n",
      "Requirement already satisfied: requests-oauthlib>=0.7.0 in /opt/app-root/lib/python3.9/site-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard~=2.6->tensorflow) (1.3.0)\n",
      "Requirement already satisfied: smmap<5,>=3.0.1 in /opt/app-root/lib/python3.9/site-packages (from gitdb<5,>=4.0.1->gitpython>=2.1.0->mlflow) (4.0.0)\n",
      "Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in /opt/app-root/lib/python3.9/site-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard~=2.6->tensorflow) (0.4.8)\n",
      "Requirement already satisfied: oauthlib>=3.0.0 in /opt/app-root/lib/python3.9/site-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard~=2.6->tensorflow) (3.1.1)\n",
      "Building wheels for collected packages: sklearn, clang, databricks-cli, alembic\n",
      "  Building wheel for sklearn (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for sklearn: filename=sklearn-0.0-py2.py3-none-any.whl size=1309 sha256=51a8224bc2818caba0445d698bb711ce8879576bb223b816b1e979b2da122df5\n",
      "  Stored in directory: /tmp/pip-ephem-wheel-cache-bgxf1888/wheels/e4/7b/98/b6466d71b8d738a0c547008b9eb39bf8676d1ff6ca4b22af1c\n",
      "  Building wheel for clang (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for clang: filename=clang-5.0-py3-none-any.whl size=30693 sha256=88f05e091b288d3b699be2f3a5ff2019fcb55f3fa610a1a8a9a1ea205ec1b821\n",
      "  Stored in directory: /tmp/pip-ephem-wheel-cache-bgxf1888/wheels/3a/ce/7a/27094f689461801c934296d07078773603663dfcaca63bb064\n",
      "  Building wheel for databricks-cli (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for databricks-cli: filename=databricks_cli-0.16.2-py3-none-any.whl size=106811 sha256=bb6e0e23f8c65f965126f9143cf7a438c3864c6dc8805571d431ff088539541b\n",
      "  Stored in directory: /tmp/pip-ephem-wheel-cache-bgxf1888/wheels/b1/e1/ad/c7f025c76afdaceefb7a32c8a380f80606ec557b152d7709ba\n",
      "  Building wheel for alembic (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for alembic: filename=alembic-1.4.1-py2.py3-none-any.whl size=158170 sha256=673f92f35e9d831b27da0d4c85983644fc57abaced19fc801a5cc76a2f0cd985\n",
      "  Stored in directory: /tmp/pip-ephem-wheel-cache-bgxf1888/wheels/05/12/70/e1473f1aab32af60e9b87f14bbbb745b7c2e86d7f3e5b5742b\n",
      "Successfully built sklearn clang databricks-cli alembic\n",
      "Installing collected packages: typeguard, tensorflow-addons, gast, clang, six, google-pasta, flatbuffers, h5py, keras-preprocessing, grpcio, opt-einsum, google-auth-oauthlib, tensorboard-data-server, tensorboard-plugin-wit, markdown, werkzeug, tensorboard, keras, typing-extensions, astunparse, tensorflow-estimator, tensorflow, sklearn, regex, nltk, cymem, murmurhash, preshed, catalogue, srsly, blis, wasabi, thinc, smart-open, pathy, spacy-legacy, spacy, textblob, gensim, databricks-cli, gunicorn, itsdangerous, Flask, prometheus-flask-exporter, docker, sqlparse, querystring-parser, python-editor, alembic, mlflow, wordcloud\n",
      "  Attempting uninstall: six\n",
      "    Found existing installation: six 1.16.0\n",
      "    Uninstalling six-1.16.0:\n",
      "      Successfully uninstalled six-1.16.0\n",
      "  Attempting uninstall: h5py\n",
      "    Found existing installation: h5py 3.3.0\n",
      "    Uninstalling h5py-3.3.0:\n",
      "      Successfully uninstalled h5py-3.3.0\n",
      "  Attempting uninstall: typing-extensions\n",
      "    Found existing installation: typing-extensions 3.10.0.0\n",
      "    Uninstalling typing-extensions-3.10.0.0:\n",
      "      Successfully uninstalled typing-extensions-3.10.0.0\n",
      "  Attempting uninstall: regex\n",
      "    Found existing installation: regex 2021.4.4\n",
      "    Uninstalling regex-2021.4.4:\n",
      "      Successfully uninstalled regex-2021.4.4\n",
      "  Attempting uninstall: alembic\n",
      "    Found existing installation: alembic 1.7.1\n",
      "    Uninstalling alembic-1.7.1:\n",
      "      Successfully uninstalled alembic-1.7.1\n",
      "\u001b[31mERROR: After October 2020 you may experience errors when installing or updating packages. This is because pip will change the way that it resolves dependency conflicts.\n",
      "\n",
      "We recommend you use --use-feature=2020-resolver to test your packages with the new resolver before it becomes the default.\n",
      "\n",
      "kfp-tekton 0.8.1 requires kfp==1.6.3, but you'll have kfp 1.8.6 which is incompatible.\n",
      "kfp-notebook 0.23.0 requires kfp==1.3.0, but you'll have kfp 1.8.6 which is incompatible.\n",
      "tensorflow 2.6.0 requires numpy~=1.19.2, but you'll have numpy 1.21.0 which is incompatible.\n",
      "flask 2.0.2 requires Jinja2>=3.0, but you'll have jinja2 2.11.3 which is incompatible.\u001b[0m\n",
      "Successfully installed Flask-2.0.2 alembic-1.4.1 astunparse-1.6.3 blis-0.7.4 catalogue-2.0.6 clang-5.0 cymem-2.0.5 databricks-cli-0.16.2 docker-5.0.3 flatbuffers-1.12 gast-0.4.0 gensim-4.1.2 google-auth-oauthlib-0.4.6 google-pasta-0.2.0 grpcio-1.41.0 gunicorn-20.1.0 h5py-3.1.0 itsdangerous-2.0.1 keras-2.6.0 keras-preprocessing-1.1.2 markdown-3.3.4 mlflow-1.20.2 murmurhash-1.0.5 nltk-3.6.5 opt-einsum-3.3.0 pathy-0.6.0 preshed-3.0.5 prometheus-flask-exporter-0.18.3 python-editor-1.0.4 querystring-parser-1.2.4 regex-2021.10.21 six-1.15.0 sklearn-0.0 smart-open-5.2.1 spacy-3.1.3 spacy-legacy-3.0.8 sqlparse-0.4.2 srsly-2.4.1 tensorboard-2.7.0 tensorboard-data-server-0.6.1 tensorboard-plugin-wit-1.8.0 tensorflow-2.6.0 tensorflow-addons-0.14.0 tensorflow-estimator-2.6.0 textblob-0.15.3 thinc-8.0.11 typeguard-2.13.0 typing-extensions-3.7.4.3 wasabi-0.8.2 werkzeug-2.0.2 wordcloud-1.8.1\n",
      "\u001b[33mWARNING: You are using pip version 20.2.4; however, version 21.3 is available.\n",
      "You should consider upgrading via the '/opt/app-root/bin/python3.9 -m pip install --upgrade pip' command.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install tensorflow_addons numpy pandas tensorflow sklearn nltk spacy textblob gensim scipy seaborn matplotlib minio mlflow wordcloud boto3\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8e8d1d55-f228-462c-ac0f-dafc2e03bfb3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/opt/app-root/src/anz_ml_project/notebooks/text_classification_notebooks'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "cwd = os.getcwd()\n",
    "\n",
    "cwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7da87e50-a13b-4621-be58-be2ba8e74011",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5fc4b5c7-a1fc-42e8-8c00-f2987f6803c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import sys\n",
    "# # sys.path is a list of absolute path strings\n",
    "# sys.path.append('/opt/app-root/src/anz_ml_project/')\n",
    "# from src.features.build_features import BuildFeatures\n",
    "# from src.modules.build_model import BuildModel\n",
    "# # from src.modules.predict_model import BuildModel\n",
    "# # from src.modules.train_model import BuildModel\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cea4e1d0-9de3-4fce-b43d-3a2463510d19",
   "metadata": {},
   "source": [
    "# Load Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "9230902c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import sys\n",
    "# # sys.path is a list of absolute path strings\n",
    "# sys.path.append('/opt/app-root/src/anz_ml_project/')\n",
    "\n",
    "# from src.features.build_features import BuildFeatures\n",
    "\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "import tensorflow_addons as tfa\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "import sklearn.feature_extraction.text as text\n",
    "from sklearn import model_selection, preprocessing, linear_model, naive_bayes, metrics, svm\n",
    "# from sklearn.naive_bayes import MultinomialNB\n",
    "# from sklearn.linear_model import LogisticRegression\n",
    "# from sklearn.ensemble import RandomForestClassifier\n",
    "# from sklearn.svm import LinearSVC\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "from textblob import TextBlob\n",
    "from nltk.stem import PorterStemmer,SnowballStemmer\n",
    "from textblob import Word\n",
    "from sklearn.feature_extraction.text import CountVectorizer,TfidfVectorizer\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "from nltk.tokenize.toktok import ToktokTokenizer\n",
    "# from wordcloud import WordCloudfrom wordcloud import WordCloud\n",
    "from io import StringIO\n",
    "import string\n",
    "# import gensim\n",
    "# from gensim.models import Word2Vec\n",
    "import itertools\n",
    "import scipy\n",
    "from scipy import spatial\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import re\n",
    "import nltk\n",
    "import joblib\n",
    "\n",
    "import mlflow\n",
    "import warnings\n",
    "\n",
    "from minio import Minio\n",
    "import subprocess\n",
    "import ipynbname\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "import absl.logging\n",
    "absl.logging.set_verbosity(absl.logging.ERROR)\n",
    "\n",
    "\n",
    "tokenizer = ToktokTokenizer()\n",
    "# stopword_list = nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b5c753d-e8f9-44b0-a54c-d08dae3f5250",
   "metadata": {},
   "source": [
    "# Define a class to read the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "524a06ba-4cf2-475f-a4f9-3f93a9270bcf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn import preprocessing\n",
    "\n",
    "class ReadData():\n",
    "    '''\n",
    "    Turn raw data into features for modeling\n",
    "    ----------\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    self.final_set:\n",
    "        Features for modeling purpose\n",
    "    self.labels:\n",
    "        Output labels of the features\n",
    "    enc: \n",
    "        Ordinal Encoder definition file\n",
    "    ohe:\n",
    "        One hot  Encoder definition file\n",
    "    '''\n",
    "    def __init__(self, CLIENT,S3BucketName = \"raw-data-saeed\",FILE_NAME=\"data.csv\", SPLIT_RATE=.2, INPUT_FEATURE_NAME ='consumer_complaint_narrative' , OUTPUT_FEATURE_NAME='product'):\n",
    "#     def __init__(self, *args, **kwargs):\n",
    "        self.file_name =  FILE_NAME \n",
    "        self.client =  CLIENT\n",
    "        self.S3BucketName = S3BucketName\n",
    "        self.split_rate = SPLIT_RATE\n",
    "        self.in_fe_name = 'consumer_complaint_narrative'\n",
    "        self.out_fe_name = 'product'\n",
    "        self.df = []\n",
    "        self.train_data = []\n",
    "        self.test_data = []\n",
    "\n",
    "        self.train_labels = []\n",
    "        self.test_labels = []\n",
    "        self.enc = []\n",
    "    \n",
    "        \n",
    "#         self.final_set,self.labels = self.build_data()\n",
    "    def ReadS3Bucket(self):\n",
    "        '''\n",
    "        Read Data from S3. bucket\n",
    "        ----------\n",
    "        \n",
    "        Returns\n",
    "        -------\n",
    "        Dataframe representation of the csv file\n",
    "        '''\n",
    "\n",
    "        csv_file = self.client.get_object(self.S3BucketName, self.file_name)\n",
    "        self.df = pd.read_csv(csv_file)\n",
    "\n",
    "        \n",
    "#         return self.df\n",
    "    ## read the data from the source file\n",
    "    def SplitData(self):\n",
    "        '''\n",
    "        Reading the csv file\n",
    "        ----------\n",
    "        \n",
    "        Returns\n",
    "        -------\n",
    "        Dataframe representation of the csv file\n",
    "        '''\n",
    "\n",
    "        self.train_data, self.test_data, self.train_labels, self.test_labels = train_test_split(self.df[self.in_fe_name], self.df[self.out_fe_name],stratify=self.df[self.out_fe_name], \n",
    "                                                    test_size=self.split_rate)\n",
    "        \n",
    "#         return self.train_data, self.test_data, self.train_labels, self.test_labels\n",
    "    def LabelEncoding(self):\n",
    "        '''\n",
    "        GetRequired Info\n",
    "        ----------\n",
    "        \n",
    "        Returns\n",
    "        -------\n",
    "        Dataframe representation of the csv file\n",
    "        '''\n",
    "        ##label encoding target variable\n",
    "        self.enc = preprocessing.LabelEncoder()\n",
    "        self.train_labels = self.enc.fit_transform(self.train_labels)\n",
    "        self.test_labels = self.enc.fit_transform(self.test_labels)\n",
    "\n",
    "        print(self.enc.classes_)\n",
    "        print(np.unique(self.train_labels, return_counts=True))\n",
    "        print(np.unique(self.test_labels, return_counts=True))\n",
    "\n",
    "\n",
    "        \n",
    "    \n",
    "    ## address the missing information\n",
    "    def ReadDataFrameData(self):\n",
    "        '''\n",
    "        Replace the missing value with the zero.\n",
    "        ----------\n",
    "        \n",
    "        Returns\n",
    "        -------\n",
    "        Dataframe with replaced missing value.\n",
    "        '''\n",
    "        self.ReadS3Bucket()\n",
    "        self.SplitData()\n",
    "        self.LabelEncoding()\n",
    "        \n",
    "        \n",
    "        \n",
    "        return self.train_data, self.test_data, self.train_labels, self.test_labels,self.enc\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9501e33e-a845-4ca3-881b-1626b363aee4",
   "metadata": {},
   "source": [
    "# Define a class to preprocess the data and make them ready for modeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "a347c7bc-8669-437f-98ec-9e2a5db15ca6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "\n",
    "class BuildFeatures():\n",
    "    '''\n",
    "    Turn raw data into features for modeling\n",
    "    ----------\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    self.final_set:\n",
    "        Features for modeling purpose\n",
    "    self.labels:\n",
    "        Output labels of the features\n",
    "    enc: \n",
    "        Ordinal Encoder definition file\n",
    "    ohe:\n",
    "        One hot  Encoder definition file\n",
    "    '''\n",
    "    def __init__(self, TRAIN_DATA,TEST_DATA,TRAIN_LABELS,TEST_LABELS, CLIENT,S3BucketName = \"raw-data-saeed\", GloveData=\"glove.6B.50d.txt\",EMBEDDING_DIM=50, WEIGHT_FLAG = False):\n",
    "#     def __init__(self, *args, **kwargs):\n",
    "        self.client =  CLIENT\n",
    "        self.S3BucketName = S3BucketName\n",
    "        self.GloveData = GloveData\n",
    "        self.EMBEDDING_DIM = EMBEDDING_DIM\n",
    "        self.word_index = []\n",
    "        self.inputs = TRAIN_DATA.values\n",
    "        self.tokenizer = Tokenizer(num_words=20000)\n",
    "        self.train_data = TRAIN_DATA.values\n",
    "        self.test_data = TEST_DATA.values\n",
    "        self.train_data_seq = []\n",
    "        self.test_data_seq = []\n",
    "        self.final_train_data = []\n",
    "        self.final_test_data = [] \n",
    "        self.train_labels = TRAIN_LABELS\n",
    "        self.test_labels = TEST_LABELS\n",
    "        self.embedding_matrix = []\n",
    "        self.MAX_SEQUENCE_LENGTH = 0\n",
    "        self.weight_flag = WEIGHT_FLAG\n",
    "        \n",
    "#         self.final_set,self.labels = self.build_data()\n",
    "    def DefineTokenizer(self):\n",
    "        '''\n",
    "        Define the To\n",
    "        ----------\n",
    "        \n",
    "        Returns\n",
    "        -------\n",
    "        Dataframe representation of the csv file\n",
    "        '''\n",
    "\n",
    "        self.tokenizer.fit_on_texts(self.inputs)#total_complaints\n",
    "\n",
    "        \n",
    "        return self.tokenizer\n",
    "    ## read the data from the source file\n",
    "    def TokenizeInputData(self):\n",
    "        '''\n",
    "        Reading the csv file\n",
    "        ----------\n",
    "        \n",
    "        Returns\n",
    "        -------\n",
    "        Dataframe representation of the csv file\n",
    "        '''\n",
    "\n",
    "        self.tokenizer = self.DefineTokenizer()\n",
    "\n",
    "        self.train_data_seq = self.tokenizer.texts_to_sequences(self.train_data)\n",
    "        self.test_data_seq = self.tokenizer.texts_to_sequences(self.test_data)\n",
    "        return self.train_data_seq, self.test_data_seq, self.tokenizer\n",
    "    def GetInfo(self):\n",
    "        '''\n",
    "        GetRequired Info\n",
    "        ----------\n",
    "        \n",
    "        Returns\n",
    "        -------\n",
    "        Dataframe representation of the csv file\n",
    "        '''\n",
    "        self.tokenizer = self.DefineTokenizer()\n",
    "        total_complaints = np.append(self.train_data,self.test_data)\n",
    "        self.MAX_SEQUENCE_LENGTH = max([len(c.split()) for c in total_complaints])\n",
    "        print('Maximum Sequence length is %s .' % self.MAX_SEQUENCE_LENGTH)\n",
    "        self.word_index = self.tokenizer.word_index# dictionary containing words and their index\n",
    "        print('Found %s unique tokens.' % len(self.word_index))\n",
    "\n",
    "        \n",
    "        return self.MAX_SEQUENCE_LENGTH,self.word_index\n",
    "    \n",
    "    ## address the missing information\n",
    "    def PaddingInputSequences(self):\n",
    "        '''\n",
    "        Replace the missing value with the zero.\n",
    "        ----------\n",
    "        \n",
    "        Returns\n",
    "        -------\n",
    "        Dataframe with replaced missing value.\n",
    "        '''\n",
    "        self.MAX_SEQUENCE_LENGTH ,self.word_index = self.GetInfo()\n",
    "        self.train_data_seq, self.test_data_seq, self.tokenizer = self.TokenizeInputData()\n",
    "        \n",
    "        \n",
    "        self.final_train_data = pad_sequences(self.train_data_seq, maxlen=self.MAX_SEQUENCE_LENGTH ,padding='post')\n",
    "        self.final_test_data = pad_sequences(self.test_data_seq, maxlen=self.MAX_SEQUENCE_LENGTH ,padding='post')\n",
    "        return self.final_train_data,self.final_test_data,self.MAX_SEQUENCE_LENGTH ,self.word_index,self.tokenizer\n",
    "\n",
    "    ## Do the label encoder on output and remove the output column from the feature vector\n",
    "    def ConvertInputLabelsToCat (self):\n",
    "        '''\n",
    "        convert input labels to categorical\n",
    "        ----------\n",
    "        \n",
    "        Returns\n",
    "        -------\n",
    "        self.data:\n",
    "            Separate features data \n",
    "        \n",
    "        self.labels:\n",
    "            Ground truth or label of feature data\n",
    "\n",
    "        \n",
    "        '''\n",
    "        ## Mapping the output to a numeric range\n",
    "        self.train_labels = to_categorical(np.asarray(self.train_labels))\n",
    "        self.test_labels = to_categorical(np.asarray(self.test_labels))\n",
    "        print('Shape of train data tensor:', self.final_train_data.shape)\n",
    "        print('Shape of train label tensor:', self.train_labels.shape)\n",
    "        print('Shape of test label tensor:', self.test_labels.shape)\n",
    "\n",
    "        return self.train_labels , self.test_labels\n",
    "\n",
    "        \n",
    "    \n",
    "\n",
    "    ## Doing ordinal encoding for the features which the order of value in the features are important\n",
    "    def LoadGloveWeights(self):\n",
    "        '''\n",
    "        ## CNN w/ Pre-trained word embeddings(GloVe)\n",
    "        We’ll use pre-trained embeddings such as Glove which provides word based vector representation trained on a large corpus.\n",
    "\n",
    "        \n",
    "        '''\n",
    "        \n",
    "        # It is trained on a dataset of one billion tokens (words) with a vocabulary of 400 thousand words. The glove has embedding vector sizes, including 50, 100, 200 and 300 dimensions.\n",
    "\n",
    "        f = self.client.get_object(self.S3BucketName, self.GloveData)\n",
    "        embeddings_index = {}\n",
    "        # f = open(os.path.join(GLOVE_DIR, 'glove.6B.50d.txt'))\n",
    "        # f = open( 'glove.6B.50d.txt')\n",
    "        for line in f:\n",
    "            # print(line.decode(\"utf-8\") )\n",
    "            line = line.decode(\"utf-8\")\n",
    "            # break\n",
    "            values = line.split()\n",
    "            word = values[0]\n",
    "            coefs = np.asarray(values[1:], dtype='float32')\n",
    "            embeddings_index[word] = coefs\n",
    "        f.close()\n",
    "\n",
    "        print('Found %s word vectors.' % len(embeddings_index))\n",
    "        \n",
    "        ## Now lets create the embedding matrix using the word indexer created from tokenizer.\n",
    "        self.embedding_matrix = np.zeros((len(self.word_index) + 1, self.EMBEDDING_DIM))\n",
    "        for word, i in self.word_index.items():\n",
    "            embedding_vector = embeddings_index.get(word)\n",
    "            if embedding_vector is not None:\n",
    "                # words not found in embedding index will be all-zeros.\n",
    "                self.embedding_matrix[i] = embedding_vector\n",
    "\n",
    "        return self.embedding_matrix\n",
    "    ## function for doing one hot encoding    \n",
    "    def PreProcessingTextData(self):\n",
    "        '''\n",
    "        Apply one hot ecoding on the string data which there order is not important, such as Gender, PaymentMethod and etc.\n",
    "        ----------\n",
    "        \n",
    "        Returns\n",
    "        -------\n",
    "        self.final_set:\n",
    "            encoded data\n",
    "        \n",
    "        ohe:\n",
    "            one hot transformer module\n",
    "        '''\n",
    "        \n",
    "\n",
    "        self.train_data_seq, self.test_data_seq, self.tokenizer = self.TokenizeInputData()\n",
    "        self.final_train_data,self.final_test_data,self.MAX_SEQUENCE_LENGTH ,self.word_index,self.tokenizer = self.PaddingInputSequences()\n",
    "        self.train_labels , self.test_labels  = self.ConvertInputLabelsToCat()\n",
    "        if self.weight_flag ==True:\n",
    "            self.embedding_matrix = self.LoadGloveWeights()\n",
    "\n",
    "    #         final_set.head(5)\n",
    "            return self.final_train_data,self.final_test_data,self.train_labels, self.test_labels,self.tokenizer,self.embedding_matrix\n",
    "        else:\n",
    "            return self.final_train_data, self.final_test_data, self.train_labels, self.test_labels, self.word_index, self.tokenizer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4fa6d2f-fb71-4758-8d83-30d77ee9ec73",
   "metadata": {},
   "source": [
    "# Define a class for building the Deep learning based model for  NLP "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "0f2f5ce7-e74f-49b4-af05-cbb0b9e2ee0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.layers import Bidirectional, Dense, Input, LSTM, Embedding\n",
    "from tensorflow.keras.models import Sequential\n",
    "\n",
    "\n",
    "\n",
    "class BuildModel():\n",
    "    '''\n",
    "    Build Lstm model for tensorflow\n",
    "    ----------\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    self.model:\n",
    "        Deep learning based Model\n",
    "    \n",
    "    '''\n",
    "    def __init__(self, WORD_INDEX, EMWEIGHTS, EMBEDDING_DIM= 50,MAX_SEQUENCE_LENGTH= 348, LOSS='categorical_crossentropy',OPTIMIZER='rmsprop',METRICS=['acc'],NUM_CLASSES=11,DROP_OUT_RATE =.4,PRE_WEIGHT_FLAG = False):\n",
    "        self.weights = [EMWEIGHTS]\n",
    "        self.input_length = MAX_SEQUENCE_LENGTH\n",
    "        self.embeding_dim = EMBEDDING_DIM\n",
    "        self.loss = LOSS\n",
    "        self.optimizer = OPTIMIZER\n",
    "        self.metrics = METRICS\n",
    "        self.model = []\n",
    "        self.num_classes = NUM_CLASSES\n",
    "        self.drate = DROP_OUT_RATE\n",
    "        self.pre_weight_flag = PRE_WEIGHT_FLAG\n",
    "        self.word_index = WORD_INDEX\n",
    "        \n",
    "    def DefineModelWithoutGLOVE(self):\n",
    "        '''\n",
    "        Define the model\n",
    "        ----------\n",
    "        \n",
    "        Returns\n",
    "        -------\n",
    "        \n",
    "        '''\n",
    "        #Bidirectional LSTM\n",
    "        self.model = Sequential()\n",
    "\n",
    "        self.model.add(Embedding(len(self.word_index) + 1,\n",
    "                                    self.embeding_dim,\n",
    "                                    input_length=self.input_length ,\n",
    "                                    trainable=True))\n",
    "        self.model.add(Bidirectional(LSTM(100, dropout = self.drate, return_sequences=True)))\n",
    "        self.model.add(Bidirectional(LSTM(256, dropout = self.drate)))\n",
    "        self.model.add(Dense(self.num_classes,activation='sigmoid'))\n",
    "        # return self.final_set,self.labels, self.enc, self.ohe,self.encoding_flag\n",
    "    def DefineModelWithGLOVE(self):\n",
    "        '''\n",
    "        Define the model\n",
    "        ----------\n",
    "        \n",
    "        Returns\n",
    "        -------\n",
    "        \n",
    "        '''\n",
    "        #Bidirectional LSTM\n",
    "        self.model = Sequential()\n",
    "        self.model.add(Embedding(len(self.word_index) + 1,\n",
    "                                    self.embeding_dim,\n",
    "                                    weights=self.weights,\n",
    "                                    input_length=self.input_length ,\n",
    "                                    trainable=True))\n",
    "        \n",
    "        self.model.add(Bidirectional(LSTM(100, dropout = self.drate, return_sequences=True)))\n",
    "        self.model.add(Bidirectional(LSTM(256, dropout = self.drate)))\n",
    "        self.model.add(Dense(self.num_classes,activation='sigmoid'))\n",
    "        return self.model\n",
    "        # return self.final_set,self.labels, self.enc, self.ohe,self.encoding_flag\n",
    "    def CompileModel(self):\n",
    "        '''\n",
    "        Compile the model\n",
    "        ----------\n",
    "        \n",
    "        Returns\n",
    "        -------\n",
    "        \n",
    "        '''\n",
    "        self.model.compile(loss=self.loss,\n",
    "              optimizer=self.optimizer,\n",
    "              metrics=self.metrics)\n",
    "#         return self.model\n",
    "    def ModelBuilding(self):\n",
    "        '''\n",
    "        Build the model\n",
    "        ----------\n",
    "        \n",
    "        Returns\n",
    "        -------\n",
    "        \n",
    "        '''\n",
    "        if self.pre_weight_flag ==True:\n",
    "            self.DefineModelWithGLOVE()\n",
    "        else:\n",
    "                \n",
    "            self.DefineModelWithoutGLOVE()\n",
    "        self.CompileModel()\n",
    "        self.model.summary()\n",
    "        return self.model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bedc612a-ca1b-4f80-9a6b-bf3ac92d35df",
   "metadata": {},
   "source": [
    "# Define a class to configur MLFLOW"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "74335132-166d-4753-973d-c627435567ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLflow():\n",
    "    '''\n",
    "    Build Lstm model for tensorflow\n",
    "    ----------\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    self.model:\n",
    "        Deep learning based Model  \n",
    "    \n",
    "    '''\n",
    "    def __init__(self, MLFLOW, HOST,EXPERIMENT_NAME):\n",
    "        self.mlflow = MLFLOW\n",
    "        self.host = HOST\n",
    "        self.experiment_name = EXPERIMENT_NAME\n",
    "        \n",
    "\n",
    "    def SetUp_Mlflow(self):\n",
    "        '''\n",
    "        Define the model\n",
    "        ----------\n",
    "        \n",
    "        Returns\n",
    "        -------\n",
    "        \n",
    "        '''       \n",
    "\n",
    "        # Connect to local MLflow tracking server\n",
    "        self.mlflow.set_tracking_uri(self.host)\n",
    "\n",
    "        # Set the experiment name...\n",
    "        self.mlflow.set_experiment(self.experiment_name)\n",
    "\n",
    "        self.mlflow.tensorflow.autolog()\n",
    "        return self.mlflow\n",
    "\n",
    "    \n",
    "\n",
    "\n",
    "    def mlflow_grid_search(methodtoexecute, methodarguments):\n",
    "        with mlflow.start_run(tags= {\n",
    "            \"mlflow.source.git.commit\" : get_git_revision_hash() ,\n",
    "            \"mlflow.user\": get_git_user(),\n",
    "            \"mlflow.source.git.repoURL\": get_git_remote(),\n",
    "            \"git_remote\": get_git_remote(),\n",
    "            \"mlflow.source.git.branch\": get_git_branch(),\n",
    "            \"mlflow.docker.image.name\": os.getenv(\"JUPYTER_IMAGE\", \"LOCAL\"),\n",
    "            \"mlflow.source.type\": \"NOTEBOOK\",\n",
    "    #         \"mlflow.source.name\": ipynbname.name()\n",
    "        }) as run:\n",
    "            methodtoexecute(**methodarguments)\n",
    "            record_details(mlflow)\n",
    "\n",
    "        return run\n",
    "    \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7cb9be3b-31d5-4077-8c05-e2dae1da877f",
   "metadata": {},
   "source": [
    "# Define a class for training the model and tracking it with MLflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "aca93390-877e-47a4-9b95-8b9ae0ed9e41",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "\n",
    "    \n",
    "\n",
    "class TrainModel():\n",
    "    '''\n",
    "    Build Lstm model for tensorflow\n",
    "    ----------\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    self.model:\n",
    "        Deep learning based Model\n",
    "    \n",
    "    '''\n",
    "    \n",
    "    def __init__(self, MODEL, MLFLOW, TOKENIZER, ENC,TRAIN_DATA, TRAIN_LABELS,TEST_DATA, TEST_LABELS,HOST, EXPERIMENT_NAME, BATCH_SIZE=64,EPOCHS=10):\n",
    "        self.model_checkpoint_callback = []\n",
    "        self.enc = ENC\n",
    "        self.tokenizer = TOKENIZER\n",
    "        self.mlflow = MLFLOW\n",
    "        self.model = MODEL\n",
    "        self.train_data = TRAIN_DATA\n",
    "        self.train_labels = TRAIN_LABELS\n",
    "        self.test_data  = TEST_DATA\n",
    "        self.test_labels = TEST_LABELS\n",
    "        self.batch_size = BATCH_SIZE\n",
    "        self.epochs = EPOCHS\n",
    "        self.host = HOST\n",
    "        self.experiment_name = EXPERIMENT_NAME\n",
    "        self.history = []\n",
    "    def get_git_revision_hash(self):\n",
    "        return subprocess.check_output(['git', 'rev-parse', 'HEAD'])\n",
    "\n",
    "    def get_git_revision_short_hash(self):\n",
    "        return subprocess.check_output(['git', 'rev-parse', '--short', 'HEAD'])\n",
    "\n",
    "    def get_git_remote(self):\n",
    "        return subprocess.check_output(['git', 'config', '--get', 'remote.origin.url'])\n",
    "\n",
    "    def get_git_user(self):\n",
    "        return subprocess.check_output(['git', 'config', 'user.name'])\n",
    "\n",
    "    def get_git_branch(self):\n",
    "        return subprocess.check_output(['git', 'branch', '--show-current'])\n",
    "\n",
    "    def get_pip_freeze(self):\n",
    "        return subprocess.check_output(['pip', 'freeze']).splitlines()\n",
    "\n",
    "\n",
    "    def record_details(self):\n",
    "        \"\"\"\n",
    "        This method is the anchor poijt and more activiteis will go in it\n",
    "        :param mlflow:\n",
    "        :return:\n",
    "        \"\"\"\n",
    "        with open(\"pip_freeze.txt\", \"wb\") as file:\n",
    "            for line in self.get_pip_freeze():\n",
    "                file.write(line)\n",
    "                file.write(bytes(\"\\n\", \"UTF-8\"))\n",
    "        self.mlflow.log_artifact(\"pip_freeze.txt\")\n",
    "        file.close()\n",
    "        self.mlflow.log_artifact(\"model.h5\", artifact_path=\"model\")\n",
    "        self.mlflow.log_artifact(\"tokenizer.pkl\", artifact_path=\"model\")\n",
    "        self.mlflow.log_artifact(\"labelencoder.pkl\", artifact_path=\"model\")\n",
    "\n",
    "        os.remove(\"pip_freeze.txt\")\n",
    "        os.remove(\"model.h5\")\n",
    "        os.remove(\"tokenizer.pkl\")\n",
    "        os.remove(\"labelencoder.pkl\")\n",
    "\n",
    "    def DefineCheckPoint(self):\n",
    "        '''\n",
    "        Define the model\n",
    "        ----------\n",
    "        \n",
    "        Returns\n",
    "        -------\n",
    "        \n",
    "        '''\n",
    "        #Bidirectional LSTM\n",
    "        checkpoint_filepath = 'model.h5'\n",
    "        self.model_checkpoint_callback = tf.keras.callbacks.ModelCheckpoint(\n",
    "            filepath=checkpoint_filepath,\n",
    "            monitor='val_acc',\n",
    "            mode='max',\n",
    "            save_best_only=True)\n",
    "        \n",
    "        \n",
    "    \n",
    "    def SavePKL(self):\n",
    "        '''\n",
    "        Define the model\n",
    "        ----------\n",
    "        \n",
    "        Returns\n",
    "        -------\n",
    "        \n",
    "        '''\n",
    "        joblib.dump(self.enc, 'labelencoder.pkl')  \n",
    "        joblib.dump(self.tokenizer, 'tokenizer.pkl')  \n",
    "\n",
    "        \n",
    "    \n",
    "        \n",
    "    def ModelTraining(self):\n",
    "        '''\n",
    "        Define the model\n",
    "        ----------\n",
    "        \n",
    "        Returns\n",
    "        -------\n",
    "        \n",
    "        '''\n",
    "        self.SavePKL()\n",
    "        self.DefineCheckPoint()\n",
    "        self.mlflow = MLflow(self.mlflow, self.host,self.experiment_name).SetUp_Mlflow()\n",
    "        with self.mlflow.start_run(tags= {\n",
    "                \"mlflow.source.git.commit\" : self.get_git_revision_hash() ,\n",
    "                \"mlflow.user\": self.get_git_user(),\n",
    "                \"mlflow.source.git.repoURL\": self.get_git_remote(),\n",
    "                \"git_remote\": self.get_git_remote(),\n",
    "                \"mlflow.source.git.branch\": self.get_git_branch(),\n",
    "                \"mlflow.docker.image.name\": os.getenv(\"JUPYTER_IMAGE\", \"LOCAL\"),\n",
    "                \"mlflow.source.type\": \"NOTEBOOK\",\n",
    "        #         \"mlflow.source.name\": ipynbname.name()\n",
    "            }) as run:\n",
    "                self.history = self.model.fit(self.train_data, self.train_labels,\n",
    "                         batch_size=self.batch_size,\n",
    "                         epochs=self.epochs,\n",
    "                         validation_data=(self.test_data, self.test_labels),callbacks=[self.model_checkpoint_callback])\n",
    "                self.record_details()\n",
    "        return self.model,self.history\n",
    "        # return self.final_set,self.labels, self.enc, self.ohe,self.encoding_flag\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff9ee528-9ae0-42d8-96fa-ee65b6b8b6e1",
   "metadata": {},
   "source": [
    "# Define classes for Deploy simulations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "35ae743c-a71d-4309-b5d4-5eb28f065ecb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import joblib\n",
    "import numpy as np\n",
    "import json\n",
    "import traceback\n",
    "import sys\n",
    "import os\n",
    "class Predictor(object):\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.loaded = False\n",
    "\n",
    "    def load(self):\n",
    "        print(\"Loading model\",os.getpid())\n",
    "        self.model = tf.keras.models.load_model('model.h5', compile=False)\n",
    "        self.labelencoder = joblib.load('labelencoder.pkl')\n",
    "        self.loaded = True\n",
    "        print(\"Loaded model\")\n",
    "\n",
    "\n",
    "\n",
    "    def predict(self, X,features_names):\n",
    "        # data = request.get(\"data\", {}).get(\"ndarray\")\n",
    "        # mult_types_array = np.array(data, dtype=object)\n",
    "        print ('step1......')\n",
    "        print(X)\n",
    "        X = tf.constant(X)\n",
    "        print ('step2......')\n",
    "        print(X)\n",
    "        if not self.loaded:\n",
    "            self.load()\n",
    "#         result = self.model.predict(X)\n",
    "        try:\n",
    "            result = self.model.predict(X)\n",
    "        except Exception as e:\n",
    "            print(traceback.format_exception(*sys.exc_info()))\n",
    "            raise # reraises the exception\n",
    "                \n",
    "        print ('step3......')\n",
    "        result = tf.sigmoid(result)\n",
    "        print ('step4......')\n",
    "        print(result)\n",
    "        result = tf.math.argmax(result,axis=1)\n",
    "        print ('step5......')\n",
    "        print(result)\n",
    "        print(result.shape)\n",
    "        \n",
    "        print(self.labelencoder.inverse_transform(result))\n",
    "        print ('step6......')\n",
    "        return json.dumps(result.numpy(), cls=JsonSerializer)\n",
    "\n",
    "class JsonSerializer(json.JSONEncoder):\n",
    "    def default(self, obj):\n",
    "        if isinstance(obj, (\n",
    "        np.int_, np.intc, np.intp, np.int8, np.int16, np.int32, np.int64, np.uint8, np.uint16, np.uint32, np.uint64)):\n",
    "            return int(obj)\n",
    "        elif isinstance(obj, (np.float_, np.float16, np.float32, np.float64)):\n",
    "            return float(obj)\n",
    "        elif isinstance(obj, (np.ndarray,)):\n",
    "            return obj.tolist()\n",
    "        return json.JSONEncoder.default(self, obj)\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "import joblib\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "import json\n",
    "\n",
    "class Transformer(object):\n",
    "    def __init__(self):\n",
    "        self.tokenizer = joblib.load('tokenizer.pkl')\n",
    "        \n",
    "    def transform_input(self, X, feature_names, meta):\n",
    "        print ('step1......')\n",
    "        print(X)\n",
    "        print(feature_names)\n",
    "\n",
    "        output = self.tokenizer.texts_to_sequences(X)\n",
    "        print ('step2......')\n",
    "        print(output)\n",
    "        \n",
    "        output = pad_sequences(output, maxlen=348,padding='post')\n",
    "        print ('step3......')\n",
    "        print(output)\n",
    "\n",
    "        return output"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02f7f669-cf8c-464c-a4b7-814cac6b408b",
   "metadata": {},
   "source": [
    "# download  artifacts for testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "ba959200-66ad-4c6b-8652-8fb59556369a",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "import os\n",
    "import mlflow\n",
    "from minio import Minio\n",
    "import openshift as oc\n",
    "from jinja2 import Template\n",
    "\n",
    "class DownloadArtifact():\n",
    "    '''\n",
    "    Build Lstm model for tensorflow\n",
    "    ----------\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    self.model:\n",
    "        Deep learning based Model  \n",
    "    \n",
    "    '''\n",
    "    def __init__(self, MLFLOW, MODEL_NAME='lstmt1', MODEL_VRSION='1',HOST = \"http://mlflow:5500\" ,MLFLOW_S3_ENDPOINT_URL = 'http://minio-ml-workshop:9000',AWS_ACCESS_KEY_ID='minio',AWS_SECRET_ACCESS_KEY='minio123',AWS_REGION='us-east-1',AWS_BUCKET_NAME ='mlflow'):\n",
    "        \n",
    "        self.mlflow = MLFLOW\n",
    "        self.model_name = MODEL_NAME\n",
    "        self.model_version = MODEL_VRSION\n",
    "        self.host = HOST\n",
    "        self.mlflow_s3_endpoint_url = MLFLOW_S3_ENDPOINT_URL\n",
    "        self.aws_access_key_id = AWS_ACCESS_KEY_ID\n",
    "        self.aws_secret_access_key = AWS_SECRET_ACCESS_KEY\n",
    "        self.aws_region = AWS_REGION\n",
    "        self.aws_bucket_name = AWS_BUCKET_NAME\n",
    "        self.SetUpOS()\n",
    "        \n",
    "    def SetUpOS(self):\n",
    "        \n",
    "\n",
    "        os.environ['MLFLOW_S3_ENDPOINT_URL'] = self.mlflow_s3_endpoint_url \n",
    "        os.environ['AWS_ACCESS_KEY_ID']= self.aws_access_key_id \n",
    "        os.environ['AWS_SECRET_ACCESS_KEY'] = self.aws_secret_access_key\n",
    "        os.environ['AWS_REGION'] = self.aws_region\n",
    "        os.environ['AWS_BUCKET_NAME']= self.aws_bucket_name\n",
    "        # os.environ['MODEL_NAME'] = 'rossdemo'\n",
    "        # os.environ['MODEL_VERSION'] = '1'\n",
    "        # os.environ['OPENSHIFT_CLIENT_PYTHON_DEFAULT_OC_PATH'] = '/tmp/oc'\n",
    "\n",
    "\n",
    "\n",
    "    def get_s3_server(self):\n",
    "        minioClient = Minio('minio-ml-workshop:9000',\n",
    "                        access_key='minio',\n",
    "                        secret_key='minio123',\n",
    "                        secure=False)\n",
    "\n",
    "        return minioClient\n",
    "\n",
    "\n",
    "    def Init_Mlfow(self):\n",
    "        self.mlflow.set_tracking_uri(self.host)\n",
    "        print(self.host)\n",
    "        # Set the experiment name...\n",
    "        #mlflow_client = mlflow.tracking.MlflowClient(HOST)\n",
    "\n",
    "\n",
    "    def download_artifacts(self):\n",
    "        print(\"retrieving model metadata from mlflow...\")\n",
    "        model = self.mlflow.pyfunc.load_model(\n",
    "            model_uri=f\"models:/{self.model_name}/{self.model_version}\"\n",
    "        )\n",
    "        print(model)\n",
    "\n",
    "        run_id = model.metadata.run_id\n",
    "        experiment_id = self.mlflow.get_run(run_id).info.experiment_id\n",
    "\n",
    "        print(\"initializing connection to s3 server...\")\n",
    "        minioClient = self.get_s3_server()\n",
    "\n",
    "    #     artifact_location = mlflow.get_experiment_by_name('rossdemo').artifact_location\n",
    "    #     print(\"downloading artifacts from s3 bucket \" + artifact_location)\n",
    "\n",
    "        data_file_model = minioClient.fget_object(\"mlflow\", f\"/{experiment_id}/{run_id}/artifacts/model/model.h5\", \"model.h5\")\n",
    "        data_file_model2 = minioClient.fget_object(\"mlflow\", f\"/{experiment_id}/{run_id}/artifacts/model/tokenizer.pkl\", \"tokenizer.pkl\")\n",
    "        data_file_model3 = minioClient.fget_object(\"mlflow\", f\"/{experiment_id}/{run_id}/artifacts/model/labelencoder.pkl\", \"labelencoder.pkl\")\n",
    "\n",
    "\n",
    "        #Using boto3 Download the files from mlflow, the file path is in the model meta\n",
    "        #write the files to the file system\n",
    "        print(\"download successful\")\n",
    "\n",
    "        return run_id"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "070e4ed1-1cad-4341-81d7-fa8780fc89b6",
   "metadata": {},
   "source": [
    "# Initialize the config file for mlflow and Minio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "6e397a69",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "HOST = \"http://mlflow:5500\"\n",
    "\n",
    "PROJECT_NAME = \"NlpTc\"\n",
    "EXPERIMENT_NAME = \"NlpLstm\"\n",
    "\n",
    "os.environ['MLFLOW_S3_ENDPOINT_URL']='http://minio-ml-workshop:9000'\n",
    "os.environ['AWS_ACCESS_KEY_ID']='minio'\n",
    "os.environ['AWS_SECRET_ACCESS_KEY']='minio123'\n",
    "os.environ['AWS_REGION']='us-east-1'\n",
    "os.environ['AWS_BUCKET_NAME']='raw-data-saeed'\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "172f2529-55bb-4a9d-acef-3358eb9b6c1a",
   "metadata": {},
   "source": [
    "\n",
    "## Define a Function to read from Minio S3 Bucket"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "e2201c73-0605-41b1-8f4b-924aae88fd44",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_s3_server():\n",
    "    minioClient = Minio('minio-ml-workshop:9000',\n",
    "                    access_key='minio',\n",
    "                    secret_key='minio123',\n",
    "                    secure=False)\n",
    "\n",
    "    return minioClient"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "bb3ae1c9-5dcb-4bca-bbe2-7d7e026b4ef0",
   "metadata": {},
   "outputs": [],
   "source": [
    "client = get_s3_server()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22deba8c-4667-4958-b114-93d5c3094fd9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "af49b7ed-6635-42d8-8981-05ec30a53d0a",
   "metadata": {},
   "source": [
    "## SetUp MLFlow to track the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "aac4f368-4c1d-4743-9dc2-6ca200f8bd09",
   "metadata": {},
   "outputs": [],
   "source": [
    "mlflow = MLflow(mlflow, HOST,EXPERIMENT_NAME).SetUp_Mlflow()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2523b21-5c01-448b-b06c-ea39181c408e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "1aabb275",
   "metadata": {},
   "source": [
    "# Readinng the data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "1c8f24db-e2bb-4dff-ba49-aa89fda5098c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Bank account or service' 'Consumer Loan' 'Credit card'\n",
      " 'Credit reporting' 'Debt collection' 'Money transfers' 'Mortgage'\n",
      " 'Other financial service' 'Payday loan' 'Prepaid card' 'Student loan']\n",
      "(array([ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10]), array([119,  90, 184, 242, 436,  22, 400,   2,  26,  10,  69]))\n",
      "(array([ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10]), array([ 30,  22,  46,  61, 109,   5, 100,   1,   7,   2,  17]))\n"
     ]
    }
   ],
   "source": [
    "train_data, test_data, train_labels, test_labels,enc = ReadData(client,S3BucketName = \"raw-data-saeed\",FILE_NAME=\"data.csv\").ReadDataFrameData()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4f116d8-378f-42ba-ae65-f6bda55a4e63",
   "metadata": {},
   "source": [
    "# Prepare data for modeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "e347a28f-63d6-4aa1-aa8e-7f24b2ce0650",
   "metadata": {},
   "outputs": [],
   "source": [
    "BFCLASS = BuildFeatures(TRAIN_DATA=train_data,TEST_DATA=test_data,TRAIN_LABELS=train_labels,TEST_LABELS=test_labels, CLIENT= client)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "9c1e5e30-daed-4acd-8da9-028c765b79ec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Maximum Sequence length is 348 .\n",
      "Found 7389 unique tokens.\n",
      "Shape of train data tensor: (1600, 348)\n",
      "Shape of train label tensor: (1600, 11)\n",
      "Shape of test label tensor: (400, 11)\n"
     ]
    }
   ],
   "source": [
    "train_data,test_data,train_labels , test_labels,word_index,tokenizer = BFCLASS.PreProcessingTextData()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "9c633ac2-5316-4a29-82f8-a17c22169f3e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'credit': 1,\n",
       " 'account': 2,\n",
       " 'payment': 3,\n",
       " 'loan': 4,\n",
       " 'would': 5,\n",
       " 'bank': 6,\n",
       " 'time': 7,\n",
       " 'told': 8,\n",
       " 'report': 9,\n",
       " 'mortgage': 10,\n",
       " 'call': 11,\n",
       " 'company': 12,\n",
       " 'debt': 13,\n",
       " 'received': 14,\n",
       " 'month': 15,\n",
       " 'information': 16,\n",
       " 'pay': 17,\n",
       " 'called': 18,\n",
       " 'year': 19,\n",
       " 'letter': 20,\n",
       " 'card': 21,\n",
       " 'day': 22,\n",
       " 'get': 23,\n",
       " 'never': 24,\n",
       " 'sent': 25,\n",
       " 'number': 26,\n",
       " 'paid': 27,\n",
       " 'back': 28,\n",
       " 'home': 29,\n",
       " 'asked': 30,\n",
       " 'service': 31,\n",
       " 'phone': 32,\n",
       " 'could': 33,\n",
       " 'said': 34,\n",
       " 'amount': 35,\n",
       " 'made': 36,\n",
       " 'due': 37,\n",
       " 'also': 38,\n",
       " 'money': 39,\n",
       " 'since': 40,\n",
       " 'collection': 41,\n",
       " 'fee': 42,\n",
       " 'u': 43,\n",
       " 'still': 44,\n",
       " 'well': 45,\n",
       " 'late': 46,\n",
       " 'make': 47,\n",
       " 'even': 48,\n",
       " 'balance': 49,\n",
       " 'charge': 50,\n",
       " 'date': 51,\n",
       " 'help': 52,\n",
       " 'document': 53,\n",
       " 'statement': 54,\n",
       " 'contracted': 55,\n",
       " 'know': 56,\n",
       " 'check': 57,\n",
       " 'modification': 58,\n",
       " 'america': 59,\n",
       " 'request': 60,\n",
       " 'issue': 61,\n",
       " 'agency': 62,\n",
       " 'interest': 63,\n",
       " 'new': 64,\n",
       " 'one': 65,\n",
       " 'name': 66,\n",
       " 'bill': 67,\n",
       " 'complaint': 68,\n",
       " 'customer': 69,\n",
       " 'process': 70,\n",
       " 'reporting': 71,\n",
       " 'file': 72,\n",
       " 'need': 73,\n",
       " 'want': 74,\n",
       " 'got': 75,\n",
       " 'last': 76,\n",
       " 'another': 77,\n",
       " 'stated': 78,\n",
       " 'send': 79,\n",
       " 'like': 80,\n",
       " 'contact': 81,\n",
       " 'chase': 82,\n",
       " 'take': 83,\n",
       " 'sale': 84,\n",
       " 'state': 85,\n",
       " 'property': 86,\n",
       " 'representative': 87,\n",
       " 'dispute': 88,\n",
       " 'please': 89,\n",
       " 'requested': 90,\n",
       " 'business': 91,\n",
       " 'trying': 92,\n",
       " 'several': 93,\n",
       " 'see': 94,\n",
       " 'foreclosure': 95,\n",
       " 'work': 96,\n",
       " 'rate': 97,\n",
       " 'going': 98,\n",
       " 'able': 99,\n",
       " 'email': 100,\n",
       " 'say': 101,\n",
       " 'financial': 102,\n",
       " 'house': 103,\n",
       " 'fraud': 104,\n",
       " 'copy': 105,\n",
       " 'bureau': 106,\n",
       " 'went': 107,\n",
       " 'consumer': 108,\n",
       " 'cargo': 109,\n",
       " 'full': 110,\n",
       " 'without': 111,\n",
       " 'however': 112,\n",
       " 'done': 113,\n",
       " 'stating': 114,\n",
       " 'attorney': 115,\n",
       " 'provide': 116,\n",
       " 'informed': 117,\n",
       " 'week': 118,\n",
       " 'filed': 119,\n",
       " 'monthly': 120,\n",
       " 'charged': 121,\n",
       " 'receive': 122,\n",
       " 'line': 123,\n",
       " 'put': 124,\n",
       " 'matter': 125,\n",
       " 'removed': 126,\n",
       " 'proof': 127,\n",
       " 'law': 128,\n",
       " 'way': 129,\n",
       " 'record': 130,\n",
       " 'notice': 131,\n",
       " 'address': 132,\n",
       " 'problem': 133,\n",
       " 'department': 134,\n",
       " 'go': 135,\n",
       " 'spoke': 136,\n",
       " 'calling': 137,\n",
       " 'system': 138,\n",
       " 'online': 139,\n",
       " 'mail': 140,\n",
       " 'someone': 141,\n",
       " 'past': 142,\n",
       " 'refused': 143,\n",
       " 'tried': 144,\n",
       " 'escort': 145,\n",
       " 'right': 146,\n",
       " 'keep': 147,\n",
       " 'provided': 148,\n",
       " 'owen': 149,\n",
       " 'closed': 150,\n",
       " 'person': 151,\n",
       " 'tax': 152,\n",
       " 'anything': 153,\n",
       " 'owe': 154,\n",
       " 'give': 155,\n",
       " 'making': 156,\n",
       " 'case': 157,\n",
       " 'different': 158,\n",
       " 'took': 159,\n",
       " 'remove': 160,\n",
       " 'court': 161,\n",
       " 'insurance': 162,\n",
       " 'already': 163,\n",
       " 'regarding': 164,\n",
       " 'signed': 165,\n",
       " 'every': 166,\n",
       " 'response': 167,\n",
       " 'claim': 168,\n",
       " 'stop': 169,\n",
       " 'find': 170,\n",
       " 'needed': 171,\n",
       " 'via': 172,\n",
       " 'nothing': 173,\n",
       " 'believe': 174,\n",
       " 'failed': 175,\n",
       " 'immediately': 176,\n",
       " '2015': 177,\n",
       " 'paying': 178,\n",
       " 'practice': 179,\n",
       " 'applied': 180,\n",
       " 'current': 181,\n",
       " 'legal': 182,\n",
       " 'sold': 183,\n",
       " 'later': 184,\n",
       " 'getting': 185,\n",
       " 'office': 186,\n",
       " 'documentation': 187,\n",
       " 'set': 188,\n",
       " 'people': 189,\n",
       " 'car': 190,\n",
       " 'order': 191,\n",
       " 'experian': 192,\n",
       " 'fund': 193,\n",
       " 'lender': 194,\n",
       " 'original': 195,\n",
       " 'many': 196,\n",
       " 'collect': 197,\n",
       " 'point': 198,\n",
       " 'reported': 199,\n",
       " 'show': 200,\n",
       " 'today': 201,\n",
       " 'e': 202,\n",
       " 'finance': 203,\n",
       " 'given': 204,\n",
       " 'note': 205,\n",
       " 'transaction': 206,\n",
       " 'supervisor': 207,\n",
       " 'use': 208,\n",
       " 'action': 209,\n",
       " 'manager': 210,\n",
       " 'advised': 211,\n",
       " 'submitted': 212,\n",
       " 'ca': 213,\n",
       " 'transfer': 214,\n",
       " 'gave': 215,\n",
       " 'additional': 216,\n",
       " 'owed': 217,\n",
       " 'personal': 218,\n",
       " '2': 219,\n",
       " 'ago': 220,\n",
       " 'finally': 221,\n",
       " 'denied': 222,\n",
       " 'contract': 223,\n",
       " 'xxxi': 224,\n",
       " 'husband': 225,\n",
       " 'equifax': 226,\n",
       " 'fact': 227,\n",
       " 'good': 228,\n",
       " 'started': 229,\n",
       " 'item': 230,\n",
       " 'security': 231,\n",
       " 'situation': 232,\n",
       " 'yet': 233,\n",
       " 'tell': 234,\n",
       " 'saying': 235,\n",
       " 'ask': 236,\n",
       " 'required': 237,\n",
       " 'reason': 238,\n",
       " 'purchase': 239,\n",
       " 'explained': 240,\n",
       " 'paperwork': 241,\n",
       " 'answer': 242,\n",
       " 'agreed': 243,\n",
       " 'capital': 244,\n",
       " 'first': 245,\n",
       " '3': 246,\n",
       " 'refuse': 247,\n",
       " 'income': 248,\n",
       " 'dollar': 249,\n",
       " 'per': 250,\n",
       " 'attached': 251,\n",
       " 'agent': 252,\n",
       " 'score': 253,\n",
       " 'used': 254,\n",
       " 'error': 255,\n",
       " 'job': 256,\n",
       " 'wife': 257,\n",
       " 'found': 258,\n",
       " 'continue': 259,\n",
       " 'within': 260,\n",
       " 'though': 261,\n",
       " 'review': 262,\n",
       " 'around': 263,\n",
       " 'left': 264,\n",
       " 'correct': 265,\n",
       " 'unable': 266,\n",
       " 'thing': 267,\n",
       " 'short': 268,\n",
       " 'feel': 269,\n",
       " 'program': 270,\n",
       " 'returned': 271,\n",
       " 'speak': 272,\n",
       " 'hour': 273,\n",
       " 'question': 274,\n",
       " 'attempt': 275,\n",
       " 'xxxxxxxx15': 276,\n",
       " 'act': 277,\n",
       " 'inquiry': 278,\n",
       " 'transferred': 279,\n",
       " 'creditor': 280,\n",
       " 'offer': 281,\n",
       " 'old': 282,\n",
       " 'end': 283,\n",
       " 'longer': 284,\n",
       " 'wanted': 285,\n",
       " 'resolve': 286,\n",
       " 'showing': 287,\n",
       " 'checking': 288,\n",
       " 'may': 289,\n",
       " 'refund': 290,\n",
       " 'asking': 291,\n",
       " 'option': 292,\n",
       " 'result': 293,\n",
       " 'working': 294,\n",
       " 'using': 295,\n",
       " 'wrong': 296,\n",
       " 'fraudulent': 297,\n",
       " 'taken': 298,\n",
       " 'boa': 299,\n",
       " 'delete': 300,\n",
       " 'written': 301,\n",
       " 'hold': 302,\n",
       " 'agreement': 303,\n",
       " 'disputed': 304,\n",
       " 'application': 305,\n",
       " 'come': 306,\n",
       " 'serving': 307,\n",
       " '30': 308,\n",
       " 'negative': 309,\n",
       " 'investigation': 310,\n",
       " 'something': 311,\n",
       " 'assistance': 312,\n",
       " 'transition': 313,\n",
       " 'period': 314,\n",
       " 'approved': 315,\n",
       " 'term': 316,\n",
       " 'vehicle': 317,\n",
       " 'telling': 318,\n",
       " 'return': 319,\n",
       " 'fix': 320,\n",
       " 'bankruptcy': 321,\n",
       " 'policy': 322,\n",
       " 'based': 323,\n",
       " 'identity': 324,\n",
       " 'rep': 325,\n",
       " 'purchased': 326,\n",
       " 'writing': 327,\n",
       " 'branch': 328,\n",
       " 'next': 329,\n",
       " 'lost': 330,\n",
       " 'place': 331,\n",
       " 'let': 332,\n",
       " 'open': 333,\n",
       " 'everything': 334,\n",
       " 'thank': 335,\n",
       " 'deposit': 336,\n",
       " 'nationstar': 337,\n",
       " 'status': 338,\n",
       " 'came': 339,\n",
       " 'receiving': 340,\n",
       " 'terrified': 341,\n",
       " 'party': 342,\n",
       " 'default': 343,\n",
       " 'federal': 344,\n",
       " 'fair': 345,\n",
       " 'and': 346,\n",
       " 'changed': 347,\n",
       " 'form': 348,\n",
       " 'behind': 349,\n",
       " 'instead': 350,\n",
       " 'previous': 351,\n",
       " 'plan': 352,\n",
       " 'available': 353,\n",
       " 'mine': 354,\n",
       " 'numerous': 355,\n",
       " 'message': 356,\n",
       " 'notified': 357,\n",
       " 'false': 358,\n",
       " 'offered': 359,\n",
       " 'sending': 360,\n",
       " 'close': 361,\n",
       " 'understand': 362,\n",
       " 'part': 363,\n",
       " 'sure': 364,\n",
       " 'history': 365,\n",
       " 'long': 366,\n",
       " 'cap': 367,\n",
       " 'receipt': 368,\n",
       " 'theft': 369,\n",
       " 'upon': 370,\n",
       " 'webster': 371,\n",
       " 'fera': 372,\n",
       " 'think': 373,\n",
       " 'paper': 374,\n",
       " 'etc': 375,\n",
       " 'try': 376,\n",
       " 'pal': 377,\n",
       " 'change': 378,\n",
       " 'listed': 379,\n",
       " 'ever': 380,\n",
       " 'total': 381,\n",
       " 'fixed': 382,\n",
       " 'verified': 383,\n",
       " 'reference': 384,\n",
       " 'according': 385,\n",
       " 'complete': 386,\n",
       " 'requesting': 387,\n",
       " 'free': 388,\n",
       " 'social': 389,\n",
       " 'continues': 390,\n",
       " 'happened': 391,\n",
       " 'kept': 392,\n",
       " 'attempted': 393,\n",
       " 'private': 394,\n",
       " 'multiple': 395,\n",
       " 'actually': 396,\n",
       " 'student': 397,\n",
       " 'xxxxxxxx2015': 398,\n",
       " 'care': 399,\n",
       " 'prior': 400,\n",
       " 'almost': 401,\n",
       " 'despite': 402,\n",
       " 'into': 403,\n",
       " 'placed': 404,\n",
       " 'access': 405,\n",
       " 'much': 406,\n",
       " 'verify': 407,\n",
       " 'title': 408,\n",
       " 'afford': 409,\n",
       " 'closing': 410,\n",
       " 'recently': 411,\n",
       " 'opened': 412,\n",
       " 'cash': 413,\n",
       " 'allowed': 414,\n",
       " 'included': 415,\n",
       " 'license': 416,\n",
       " 'schedule': 417,\n",
       " 'dated': 418,\n",
       " 'city': 419,\n",
       " 'following': 420,\n",
       " 'either': 421,\n",
       " '5': 422,\n",
       " 'always': 423,\n",
       " 'allow': 424,\n",
       " 'settlement': 425,\n",
       " '100000': 426,\n",
       " '2014': 427,\n",
       " 'violation': 428,\n",
       " 'principal': 429,\n",
       " 'accept': 430,\n",
       " 'lower': 431,\n",
       " 'start': 432,\n",
       " 'really': 433,\n",
       " 'deal': 434,\n",
       " 'resolved': 435,\n",
       " 'least': 436,\n",
       " 'automatic': 437,\n",
       " 'canceled': 438,\n",
       " 'collector': 439,\n",
       " 'family': 440,\n",
       " 'currently': 441,\n",
       " '15': 442,\n",
       " 'cost': 443,\n",
       " 'clear': 444,\n",
       " 'thought': 445,\n",
       " 'alleged': 446,\n",
       " 'explain': 447,\n",
       " 'including': 448,\n",
       " 'missed': 449,\n",
       " 'debit': 450,\n",
       " 'must': 451,\n",
       " 'annual': 452,\n",
       " '6': 453,\n",
       " 'lawyer': 454,\n",
       " '7': 455,\n",
       " 'repeatedly': 456,\n",
       " 'cancel': 457,\n",
       " 'bad': 458,\n",
       " 'filling': 459,\n",
       " 'responded': 460,\n",
       " 'began': 461,\n",
       " 'recovery': 462,\n",
       " 'charging': 463,\n",
       " 'delinquent': 464,\n",
       " 'along': 465,\n",
       " 'concern': 466,\n",
       " 'continued': 467,\n",
       " 'auto': 468,\n",
       " 'look': 469,\n",
       " 'away': 470,\n",
       " 'filing': 471,\n",
       " 'release': 472,\n",
       " 'knowledge': 473,\n",
       " 'heard': 474,\n",
       " 'hard': 475,\n",
       " 'type': 476,\n",
       " 'therefore': 477,\n",
       " 'incorrect': 478,\n",
       " 'telephone': 479,\n",
       " 'talk': 480,\n",
       " '4': 481,\n",
       " 'else': 482,\n",
       " 'created': 483,\n",
       " 'mistake': 484,\n",
       " 'price': 485,\n",
       " 'contracting': 486,\n",
       " 'trust': 487,\n",
       " 'citibank': 488,\n",
       " 'enough': 489,\n",
       " 'life': 490,\n",
       " 'decided': 491,\n",
       " 'harassment': 492,\n",
       " 'county': 493,\n",
       " 'communication': 494,\n",
       " 'high': 495,\n",
       " 'idea': 496,\n",
       " 'direct': 497,\n",
       " 'increase': 498,\n",
       " 'holder': 499,\n",
       " 'best': 500,\n",
       " 'buy': 501,\n",
       " 'recorded': 502,\n",
       " 'loss': 503,\n",
       " 'borrowed': 504,\n",
       " 'ardor': 505,\n",
       " 'third': 506,\n",
       " 'payoff': 507,\n",
       " 'son': 508,\n",
       " 'minute': 509,\n",
       " 'held': 510,\n",
       " 'anyone': 511,\n",
       " 'reply': 512,\n",
       " 'portfolio': 513,\n",
       " 'aware': 514,\n",
       " 'plus': 515,\n",
       " 'completed': 516,\n",
       " 'approximately': 517,\n",
       " 'two': 518,\n",
       " 'qualify': 519,\n",
       " 'giving': 520,\n",
       " 'sign': 521,\n",
       " 'apply': 522,\n",
       " 'attempting': 523,\n",
       " 'hope': 524,\n",
       " 'conversation': 525,\n",
       " 'seems': 526,\n",
       " 'caused': 527,\n",
       " 'ratification': 528,\n",
       " 'hung': 529,\n",
       " 'purification': 530,\n",
       " 'investigate': 531,\n",
       " 'issued': 532,\n",
       " 'supposed': 533,\n",
       " 'move': 534,\n",
       " 'three': 535,\n",
       " 'government': 536,\n",
       " 'bought': 537,\n",
       " 'local': 538,\n",
       " 'express': 539,\n",
       " 'limit': 540,\n",
       " 'for': 541,\n",
       " 'mean': 542,\n",
       " 'addition': 543,\n",
       " 'minimum': 544,\n",
       " 'inaccurate': 545,\n",
       " 'approval': 546,\n",
       " 'possible': 547,\n",
       " 'illegal': 548,\n",
       " '1': 549,\n",
       " 'respond': 550,\n",
       " 'le': 551,\n",
       " 'value': 552,\n",
       " 'associate': 553,\n",
       " 'victim': 554,\n",
       " 'run': 555,\n",
       " 'owner': 556,\n",
       " 'final': 557,\n",
       " 'spy': 558,\n",
       " 'entire': 559,\n",
       " 'forbearance': 560,\n",
       " 'sofa': 561,\n",
       " 'submit': 562,\n",
       " 'banking': 563,\n",
       " 'simply': 564,\n",
       " 'authorized': 565,\n",
       " 'wait': 566,\n",
       " 'patient': 567,\n",
       " 'voice': 568,\n",
       " 'posted': 569,\n",
       " 'entrust': 570,\n",
       " 'assured': 571,\n",
       " 'resolution': 572,\n",
       " 'live': 573,\n",
       " 'signature': 574,\n",
       " 'financing': 575,\n",
       " 'noticed': 576,\n",
       " 'reward': 577,\n",
       " 'alert': 578,\n",
       " 'xxxx2015': 579,\n",
       " 'claimed': 580,\n",
       " 'turned': 581,\n",
       " '10': 582,\n",
       " 'taking': 583,\n",
       " 'forced': 584,\n",
       " 'repeated': 585,\n",
       " 'obligation': 586,\n",
       " 'prove': 587,\n",
       " 'medical': 588,\n",
       " 'soon': 589,\n",
       " 'll': 590,\n",
       " 'effort': 591,\n",
       " 'behalf': 592,\n",
       " 'evidence': 593,\n",
       " 'clearly': 594,\n",
       " 'corrected': 595,\n",
       " 'union': 596,\n",
       " 'worked': 597,\n",
       " 'none': 598,\n",
       " 'speaking': 599,\n",
       " 'early': 600,\n",
       " 'checked': 601,\n",
       " 'promised': 602,\n",
       " 'employee': 603,\n",
       " 'ally': 604,\n",
       " 'tactic': 605,\n",
       " 'validity': 606,\n",
       " 'remained': 607,\n",
       " 'responsible': 608,\n",
       " 'confirmed': 609,\n",
       " 'happen': 610,\n",
       " 'management': 611,\n",
       " 'cell': 612,\n",
       " 'discovered': 613,\n",
       " 'became': 614,\n",
       " 'authorization': 615,\n",
       " 'judgment': 616,\n",
       " 'looking': 617,\n",
       " 'unfair': 618,\n",
       " 'reached': 619,\n",
       " 'camp': 620,\n",
       " 'remaining': 621,\n",
       " \"i'd\": 622,\n",
       " 'occasion': 623,\n",
       " 'activity': 624,\n",
       " 'explanation': 625,\n",
       " 'decision': 626,\n",
       " 'damage': 627,\n",
       " 'daughter': 628,\n",
       " 'extra': 629,\n",
       " 'talked': 630,\n",
       " 'cease': 631,\n",
       " '100': 632,\n",
       " 'previously': 633,\n",
       " 'green': 634,\n",
       " 'tree': 635,\n",
       " 'member': 636,\n",
       " 'school': 637,\n",
       " 'outstanding': 638,\n",
       " 'support': 639,\n",
       " 'lending': 640,\n",
       " 'repayment': 641,\n",
       " 'beginning': 642,\n",
       " 'protection': 643,\n",
       " 'lot': 644,\n",
       " 'completely': 645,\n",
       " 'far': 646,\n",
       " 'wrote': 647,\n",
       " 'added': 648,\n",
       " 'rude': 649,\n",
       " 'small': 650,\n",
       " 'withdrawal': 651,\n",
       " 'sell': 652,\n",
       " 'stopped': 653,\n",
       " 'directly': 654,\n",
       " 'variation': 655,\n",
       " 'certain': 656,\n",
       " 'moved': 657,\n",
       " 'solution': 658,\n",
       " 'arrangement': 659,\n",
       " 'saving': 660,\n",
       " 'certificate': 661,\n",
       " 'better': 662,\n",
       " 'guideline': 663,\n",
       " 'manner': 664,\n",
       " 'knew': 665,\n",
       " 'california': 666,\n",
       " 'child': 667,\n",
       " 'friday': 668,\n",
       " 'appraisal': 669,\n",
       " 'cover': 670,\n",
       " 'package': 671,\n",
       " 'faith': 672,\n",
       " 'timely': 673,\n",
       " 'client': 674,\n",
       " 'living': 675,\n",
       " 'responsibility': 676,\n",
       " 'hardship': 677,\n",
       " 'real': 678,\n",
       " 'showed': 679,\n",
       " 'bonus': 680,\n",
       " 'firm': 681,\n",
       " 'turn': 682,\n",
       " 'read': 683,\n",
       " 'code': 684,\n",
       " 'face': 685,\n",
       " 'avoid': 686,\n",
       " 'dealing': 687,\n",
       " 'assigned': 688,\n",
       " 'detail': 689,\n",
       " 'post': 690,\n",
       " 'fault': 691,\n",
       " 'single': 692,\n",
       " 'lawsuit': 693,\n",
       " 'condition': 694,\n",
       " 'appears': 695,\n",
       " 'promotion': 696,\n",
       " 'requirement': 697,\n",
       " 'confirmation': 698,\n",
       " 'lose': 699,\n",
       " 'mode': 700,\n",
       " 'regard': 701,\n",
       " '50000': 702,\n",
       " 'judgement': 703,\n",
       " 'public': 704,\n",
       " 'mother': 705,\n",
       " 'violated': 706,\n",
       " 'leave': 707,\n",
       " 'course': 708,\n",
       " 'subject': 709,\n",
       " 'denial': 710,\n",
       " 'police': 711,\n",
       " 'low': 712,\n",
       " 'buyer': 713,\n",
       " 'update': 714,\n",
       " 'accepted': 715,\n",
       " 'lack': 716,\n",
       " 'stay': 717,\n",
       " 'whole': 718,\n",
       " 'followed': 719,\n",
       " 'regular': 720,\n",
       " 'increased': 721,\n",
       " 'valid': 722,\n",
       " 'till': 723,\n",
       " 'add': 724,\n",
       " 'proper': 725,\n",
       " 'research': 726,\n",
       " 'waiting': 727,\n",
       " 'gone': 728,\n",
       " 'active': 729,\n",
       " 'obtain': 730,\n",
       " 'deed': 731,\n",
       " 'belongs': 732,\n",
       " 'accurate': 733,\n",
       " 'thanks': 734,\n",
       " 'named': 735,\n",
       " 'rating': 736,\n",
       " 'follow': 737,\n",
       " 'willing': 738,\n",
       " 'bring': 739,\n",
       " 'answered': 740,\n",
       " 'kind': 741,\n",
       " 'correspondence': 742,\n",
       " 'indicated': 743,\n",
       " 'saw': 744,\n",
       " 'known': 745,\n",
       " 'forward': 746,\n",
       " 'talking': 747,\n",
       " 'second': 748,\n",
       " 'driver': 749,\n",
       " 'financed': 750,\n",
       " 'deposited': 751,\n",
       " 'higher': 752,\n",
       " 'supporting': 753,\n",
       " 'updated': 754,\n",
       " 'store': 755,\n",
       " 'benefit': 756,\n",
       " 'a': 757,\n",
       " 'pending': 758,\n",
       " 'although': 759,\n",
       " 'claiming': 760,\n",
       " 'confirm': 761,\n",
       " 'hired': 762,\n",
       " 'write': 763,\n",
       " 'employment': 764,\n",
       " 'trial': 765,\n",
       " 'erroneous': 766,\n",
       " 'concerning': 767,\n",
       " 'harassing': 768,\n",
       " 'scan': 769,\n",
       " 'entry': 770,\n",
       " 'computer': 771,\n",
       " 'initial': 772,\n",
       " 'considered': 773,\n",
       " 'enter': 774,\n",
       " 'story': 775,\n",
       " 'veteran': 776,\n",
       " 'word': 777,\n",
       " 'page': 778,\n",
       " 'homeowner': 779,\n",
       " 'totally': 780,\n",
       " 'declined': 781,\n",
       " 'spent': 782,\n",
       " 'ignored': 783,\n",
       " 'citimortgage': 784,\n",
       " 'seem': 785,\n",
       " 'provider': 786,\n",
       " 'data': 787,\n",
       " 'towards': 788,\n",
       " 'extension': 789,\n",
       " 'professional': 790,\n",
       " 'ordered': 791,\n",
       " 'area': 792,\n",
       " 'reflect': 793,\n",
       " 'notify': 794,\n",
       " 'sometimes': 795,\n",
       " 'standing': 796,\n",
       " 'yes': 797,\n",
       " 'difficult': 798,\n",
       " 'fdcpa': 799,\n",
       " 'refusing': 800,\n",
       " 'especially': 801,\n",
       " 'unfortunately': 802,\n",
       " 'reach': 803,\n",
       " 'correctly': 804,\n",
       " 'cancellation': 805,\n",
       " 'trustee': 806,\n",
       " 'wish': 807,\n",
       " 'deceptive': 808,\n",
       " 'discus': 809,\n",
       " 'freeze': 810,\n",
       " 'inform': 811,\n",
       " 'hand': 812,\n",
       " 'expense': 813,\n",
       " 'others': 814,\n",
       " 'ha': 815,\n",
       " 'advance': 816,\n",
       " 'eligible': 817,\n",
       " 'mon': 818,\n",
       " 'island': 819,\n",
       " 'ended': 820,\n",
       " 'reduced': 821,\n",
       " 'passed': 822,\n",
       " 'national': 823,\n",
       " 'litigation': 824,\n",
       " 'disclosure': 825,\n",
       " 'apartment': 826,\n",
       " 'lease': 827,\n",
       " 'looked': 828,\n",
       " 'overdraft': 829,\n",
       " 'calibre': 830,\n",
       " 'permission': 831,\n",
       " 'unauthorized': 832,\n",
       " 'moving': 833,\n",
       " 'attention': 834,\n",
       " 'pulled': 835,\n",
       " 'actual': 836,\n",
       " 'recent': 837,\n",
       " 'rest': 838,\n",
       " 'apparently': 839,\n",
       " 'proceeding': 840,\n",
       " 'purpose': 841,\n",
       " 'threatened': 842,\n",
       " 'rather': 843,\n",
       " 'worth': 844,\n",
       " 'little': 845,\n",
       " 'referred': 846,\n",
       " 'corporation': 847,\n",
       " 'penalized': 848,\n",
       " 'equity': 849,\n",
       " 'applying': 850,\n",
       " 'shortage': 851,\n",
       " 'providing': 852,\n",
       " 'threatening': 853,\n",
       " 'demanding': 854,\n",
       " 'depot': 855,\n",
       " 'ability': 856,\n",
       " 'thus': 857,\n",
       " 'chapter': 858,\n",
       " 'officer': 859,\n",
       " 'seen': 860,\n",
       " 'reviewed': 861,\n",
       " 'april': 862,\n",
       " 'rental': 863,\n",
       " 'employer': 864,\n",
       " 'separate': 865,\n",
       " 'demand': 866,\n",
       " 'worse': 867,\n",
       " 'honor': 868,\n",
       " 'rule': 869,\n",
       " 'putting': 870,\n",
       " 'repaid': 871,\n",
       " 'originally': 872,\n",
       " 'mi': 873,\n",
       " 'neither': 874,\n",
       " 'related': 875,\n",
       " 'analysis': 876,\n",
       " 'acceptance': 877,\n",
       " 'indicating': 878,\n",
       " '000': 879,\n",
       " 'individual': 880,\n",
       " 'block': 881,\n",
       " 'acknowledge': 882,\n",
       " 'uterus': 883,\n",
       " 'brought': 884,\n",
       " 'alone': 885,\n",
       " 'effect': 886,\n",
       " 'remain': 887,\n",
       " 'night': 888,\n",
       " 'statute': 889,\n",
       " 'legally': 890,\n",
       " 'relief': 891,\n",
       " 'coming': 892,\n",
       " 'automatically': 893,\n",
       " 'major': 894,\n",
       " 'risk': 895,\n",
       " 'possession': 896,\n",
       " 'opinion': 897,\n",
       " 'daily': 898,\n",
       " 'ethical': 899,\n",
       " 'couple': 900,\n",
       " 'knowing': 901,\n",
       " '300000': 902,\n",
       " 'list': 903,\n",
       " 'citizen': 904,\n",
       " 'unpaid': 905,\n",
       " 'authority': 906,\n",
       " 'advise': 907,\n",
       " 'spoken': 908,\n",
       " 'reverse': 909,\n",
       " 'occurred': 910,\n",
       " 'consigned': 911,\n",
       " 'fine': 912,\n",
       " 'mention': 913,\n",
       " 'committed': 914,\n",
       " 'cleared': 915,\n",
       " 'discover': 916,\n",
       " 'recording': 917,\n",
       " 'section': 918,\n",
       " 'instruction': 919,\n",
       " 'location': 920,\n",
       " 'attended': 921,\n",
       " 'owned': 922,\n",
       " 'seller': 923,\n",
       " 'missing': 924,\n",
       " 'true': 925,\n",
       " 'intent': 926,\n",
       " 'desist': 927,\n",
       " 'product': 928,\n",
       " 'future': 929,\n",
       " 'properly': 930,\n",
       " 'limitation': 931,\n",
       " '2500': 932,\n",
       " 'florida': 933,\n",
       " 'sac': 934,\n",
       " 'processing': 935,\n",
       " '10000': 936,\n",
       " 'assignment': 937,\n",
       " 'power': 938,\n",
       " 'include': 939,\n",
       " 'impossible': 940,\n",
       " 'killed': 941,\n",
       " '12': 942,\n",
       " 'except': 943,\n",
       " 'starting': 944,\n",
       " 'anymore': 945,\n",
       " 'deletion': 946,\n",
       " 'lived': 947,\n",
       " 'possessed': 948,\n",
       " 'land': 949,\n",
       " 'investor': 950,\n",
       " 'expired': 951,\n",
       " 'meeting': 952,\n",
       " 'bayview': 953,\n",
       " 'market': 954,\n",
       " 'hear': 955,\n",
       " 'gotten': 956,\n",
       " 'experience': 957,\n",
       " 'funding': 958,\n",
       " 'assist': 959,\n",
       " 'event': 960,\n",
       " 'morning': 961,\n",
       " 'apex': 962,\n",
       " 'dealer': 963,\n",
       " 'sue': 964,\n",
       " 'lie': 965,\n",
       " 'suit': 966,\n",
       " 'save': 967,\n",
       " 'leaving': 968,\n",
       " 'front': 969,\n",
       " 'choice': 970,\n",
       " 'collecting': 971,\n",
       " 'lead': 972,\n",
       " 'dropped': 973,\n",
       " 'instructed': 974,\n",
       " 'figure': 975,\n",
       " 'position': 976,\n",
       " 'initially': 977,\n",
       " 'stress': 978,\n",
       " 'method': 979,\n",
       " 'residence': 980,\n",
       " 'discharged': 981,\n",
       " 'internet': 982,\n",
       " 'might': 983,\n",
       " 'basis': 984,\n",
       " 'monday': 985,\n",
       " 'extremely': 986,\n",
       " 'relationship': 987,\n",
       " 'difference': 988,\n",
       " 'consent': 989,\n",
       " 'foreclose': 990,\n",
       " '40000': 991,\n",
       " 'specifically': 992,\n",
       " 'failure': 993,\n",
       " 'had': 994,\n",
       " 'furthermore': 995,\n",
       " 'realized': 996,\n",
       " 'failing': 997,\n",
       " 'portion': 998,\n",
       " '30000': 999,\n",
       " 'together': 1000,\n",
       " ...}"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_index"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f577c518",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# Deep Learning define, train and test model\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "752ffe69-6213-4614-aa12-32b1cbdfd885",
   "metadata": {},
   "source": [
    "## Define the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "b740adae-5c4f-4d0c-90ef-b8cfb9da8720",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding (Embedding)        (None, 348, 50)           369500    \n",
      "_________________________________________________________________\n",
      "bidirectional (Bidirectional (None, 348, 200)          120800    \n",
      "_________________________________________________________________\n",
      "bidirectional_1 (Bidirection (None, 512)               935936    \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 11)                5643      \n",
      "=================================================================\n",
      "Total params: 1,431,879\n",
      "Trainable params: 1,431,879\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = BuildModel(WORD_INDEX=word_index, EMWEIGHTS=[]).ModelBuilding()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c37d919-de54-4565-a4b6-74a58e4f1329",
   "metadata": {},
   "source": [
    "## Train the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df9aca2b-be70-4ca2-b89d-c33df03f2d3c",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3\n",
      "25/25 [==============================] - ETA: 3:12 - loss: 2.3993 - acc: 0.203 - ETA: 33s - loss: 2.3566 - acc: 0.210 - ETA: 41s - loss: 2.3779 - acc: 0.21 - ETA: 34s - loss: 2.3491 - acc: 0.23 - ETA: 30s - loss: 2.3142 - acc: 0.23 - ETA: 27s - loss: 2.2827 - acc: 0.25 - ETA: 25s - loss: 2.2298 - acc: 0.25 - ETA: 23s - loss: 2.1943 - acc: 0.25 - ETA: 21s - loss: 2.1648 - acc: 0.26 - ETA: 19s - loss: 2.1343 - acc: 0.27 - ETA: 18s - loss: 2.1254 - acc: 0.27 - ETA: 16s - loss: 2.1166 - acc: 0.26 - ETA: 15s - loss: 2.0977 - acc: 0.27 - ETA: 13s - loss: 2.0810 - acc: 0.27 - ETA: 12s - loss: 2.0763 - acc: 0.27 - ETA: 11s - loss: 2.0647 - acc: 0.27 - ETA: 9s - loss: 2.0429 - acc: 0.2868 - ETA: 8s - loss: 2.0331 - acc: 0.292 - ETA: 7s - loss: 2.0233 - acc: 0.292 - ETA: 6s - loss: 1.9997 - acc: 0.305 - ETA: 4s - loss: 2.0140 - acc: 0.308 - ETA: 3s - loss: 1.9993 - acc: 0.311 - ETA: 2s - loss: 1.9855 - acc: 0.315 - ETA: 1s - loss: 1.9771 - acc: 0.318 - ETA: 0s - loss: 1.9630 - acc: 0.323 - 41s 1s/step - loss: 1.9630 - acc: 0.3237 - val_loss: 1.5246 - val_acc: 0.4675\n",
      "Epoch 2/3\n",
      "25/25 [==============================] - ETA: 26s - loss: 1.5831 - acc: 0.42 - ETA: 25s - loss: 1.5687 - acc: 0.46 - ETA: 24s - loss: 1.4340 - acc: 0.50 - ETA: 23s - loss: 1.4812 - acc: 0.49 - ETA: 22s - loss: 1.4625 - acc: 0.47 - ETA: 21s - loss: 1.4155 - acc: 0.49 - ETA: 20s - loss: 1.4350 - acc: 0.48 - ETA: 19s - loss: 1.4504 - acc: 0.48 - ETA: 18s - loss: 1.4550 - acc: 0.47 - ETA: 17s - loss: 1.4539 - acc: 0.47 - ETA: 15s - loss: 1.4394 - acc: 0.48 - ETA: 14s - loss: 1.4225 - acc: 0.49 - ETA: 13s - loss: 1.4291 - acc: 0.48 - ETA: 12s - loss: 1.4185 - acc: 0.48 - ETA: 11s - loss: 1.4106 - acc: 0.48 - ETA: 10s - loss: 1.4076 - acc: 0.49 - ETA: 9s - loss: 1.4076 - acc: 0.4945 - ETA: 7s - loss: 1.4181 - acc: 0.489 - ETA: 6s - loss: 1.4136 - acc: 0.489 - ETA: 5s - loss: 1.4113 - acc: 0.487 - ETA: 4s - loss: 1.4209 - acc: 0.485 - ETA: 3s - loss: 1.4113 - acc: 0.489 - ETA: 2s - loss: 1.4146 - acc: 0.490 - ETA: 1s - loss: 1.4109 - acc: 0.494 - ETA: 0s - loss: 1.4025 - acc: 0.496 - 31s 1s/step - loss: 1.4025 - acc: 0.4969 - val_loss: 1.2916 - val_acc: 0.5300\n",
      "Epoch 3/3\n",
      "25/25 [==============================] - ETA: 27s - loss: 1.4440 - acc: 0.42 - ETA: 26s - loss: 1.4952 - acc: 0.55 - ETA: 25s - loss: 1.4647 - acc: 0.54 - ETA: 24s - loss: 1.3674 - acc: 0.57 - ETA: 22s - loss: 1.2903 - acc: 0.58 - ETA: 21s - loss: 1.2520 - acc: 0.58 - ETA: 20s - loss: 1.2761 - acc: 0.57 - ETA: 19s - loss: 1.2722 - acc: 0.56 - ETA: 18s - loss: 1.2801 - acc: 0.55 - ETA: 17s - loss: 1.2702 - acc: 0.55 - ETA: 15s - loss: 1.2727 - acc: 0.56 - ETA: 14s - loss: 1.2608 - acc: 0.56 - ETA: 13s - loss: 1.2457 - acc: 0.56 - ETA: 12s - loss: 1.2314 - acc: 0.57 - ETA: 11s - loss: 1.2305 - acc: 0.56 - ETA: 10s - loss: 1.2307 - acc: 0.56 - ETA: 9s - loss: 1.2155 - acc: 0.5744 - ETA: 7s - loss: 1.2137 - acc: 0.574 - ETA: 6s - loss: 1.1983 - acc: 0.580 - ETA: 5s - loss: 1.1819 - acc: 0.589 - ETA: 4s - loss: 1.1813 - acc: 0.591 - ETA: 3s - loss: 1.1724 - acc: 0.594 - ETA: 2s - loss: 1.1709 - acc: 0.593 - ETA: 1s - loss: 1.1731 - acc: 0.589 - ETA: 0s - loss: 1.1648 - acc: 0.594 - 30s 1s/step - loss: 1.1648 - acc: 0.5944 - val_loss: 1.1268 - val_acc: 0.6675\n",
      "INFO:tensorflow:Assets written to: /tmp/tmp70p0iewl/model/data/model/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: /tmp/tmp70p0iewl/model/data/model/assets\n"
     ]
    }
   ],
   "source": [
    "model, history= TrainModel(model, mlflow, tokenizer, enc,train_data, train_labels,test_data, test_labels,HOST, EXPERIMENT_NAME, BATCH_SIZE=64,EPOCHS=3).ModelTraining()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08d0644c-f188-43c9-a17c-c327145a641e",
   "metadata": {},
   "source": [
    "### Plot the training and testing Loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9428b35",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig1 = plt.figure()\n",
    "plt.plot(history.history['loss'],'r',linewidth=3.0)\n",
    "plt.plot(history.history['val_loss'],'b',linewidth=3.0)\n",
    "plt.legend(['Training loss', 'Validation Loss'],fontsize=18)\n",
    "plt.xlabel('Epochs ',fontsize=16)\n",
    "plt.ylabel('Loss',fontsize=16)\n",
    "plt.title('Loss Curves :RNN - LSTM',fontsize=16)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f7712ce-2e36-476e-80fa-68b568cb52f5",
   "metadata": {},
   "source": [
    "#### Plot the training and testing Accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d876903",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig1 = plt.figure()\n",
    "plt.plot(history.history['acc'],'r',linewidth=3.0)\n",
    "plt.plot(history.history['val_acc'],'b',linewidth=3.0)\n",
    "plt.legend(['Training acc', 'Validation acc'],fontsize=18)\n",
    "plt.xlabel('Epochs ',fontsize=16)\n",
    "plt.ylabel('Accuracy',fontsize=16)\n",
    "plt.title('Accuracy Curves :RNN - LSTM',fontsize=16)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e411f40-3785-486c-8db3-0db25e85f9a9",
   "metadata": {},
   "source": [
    "## Test the model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d71c9850-8bfe-42c5-839f-2509197ca1b3",
   "metadata": {},
   "source": [
    "### Download the artifacts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d516f5b-c95f-46f5-93c4-21f7b01e2d46",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "DownloadArtifact(mlflow, MODEL_NAME='lstmt1', MODEL_VRSION='1').download_artifacts()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0facfe75-65cd-47fd-b06a-dbc61865aa4c",
   "metadata": {},
   "source": [
    "###  Test with Actual  data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "188c78cd-dcdd-4e80-b781-45f7942617c8",
   "metadata": {},
   "source": [
    "#### Define a sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d99c611-218f-4c0f-b9d5-7ad65a4f9b72",
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_data = {\"data\":\n",
    "  {\n",
    "\n",
    "\n",
    "        \"names\":\n",
    "            [\n",
    "              \"Debt collection\"\n",
    "            ],\n",
    "      \"ndarray\": [\"could longer pay enormous charge hired company nl take either nothing pay day loan company accept term get several letter week threatened take civil action get check\"]\n",
    "\n",
    "  }\n",
    "}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04193d4a-0420-4779-8fdd-1e5b057140d7",
   "metadata": {},
   "source": [
    "#### Transform the data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b8b28e8-ab1c-4720-b50e-6d43517b9a9b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "ready_data = Transformer().transform_input(sample_data,\"name\",\"meta\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cdc8f20a-a765-4a04-8003-967d83581a9d",
   "metadata": {},
   "source": [
    "### Test the prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19d99aca-5668-4793-a4c7-fe59cac7768e",
   "metadata": {},
   "outputs": [],
   "source": [
    "output = Predictor().predict(ready_data,\"features\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d90c426e-40b7-4b79-90f3-fa8fab06010d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "994de0ac-7ef5-40c6-96c0-d2711897767f",
   "metadata": {},
   "outputs": [],
   "source": [
    "output = Predictor().predict(ready_data,ready_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afd8dba8-b327-469a-a2b9-95b52e40dff3",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.predict(ready_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d89ef56e",
   "metadata": {},
   "outputs": [],
   "source": [
    "history = model.fit(train_data, labels_train,\n",
    " batch_size=64,\n",
    " epochs=80,\n",
    " validation_data=(test_data, labels_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c922df7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e61836da",
   "metadata": {},
   "outputs": [],
   "source": [
    "#predictions on test data\n",
    "predicted=model.predict(test_data)\n",
    "predicted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89568132",
   "metadata": {},
   "outputs": [],
   "source": [
    "#model evaluation\n",
    "import sklearn\n",
    "from sklearn.metrics import precision_recall_fscore_support as score\n",
    "precision, recall, fscore, support = score(labels_test, predicted.round())\n",
    "print('precision: \\n{}'.format(precision))\n",
    "print('recall: \\n{}'.format(recall))\n",
    "print('fscore: \\n{}'.format(fscore))\n",
    "print('support: \\n{}'.format(support))\n",
    "print(\"############################\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5c09ebb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc0558f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(classification_report(labels_test, predicted.round(),target_names=df1['product'].unique()))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a92b61f",
   "metadata": {},
   "source": [
    "After hours of training we get good results with LSTM(type of recurrent neural network) compared to CNN. From the learning curves it is clear the model needs to be tuned for overfitting by selecting hyperparameters such as no of epochs via early stopping and dropout for regularization.\n",
    "\n",
    "We could further improve our final result by ensembling our xgboost and Neural network models by using Logistic Regression as our base model.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a35fdb6-cc8c-42ca-a15d-6815de5f7538",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "10d220c55dad87bc296f91a276cffc08c658df4f112171a2982baf6251a25e4e"
  },
  "kernelspec": {
   "display_name": "Python 3.9 (XPython)",
   "language": "python",
   "name": "xpython"
  },
  "language_info": {
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
