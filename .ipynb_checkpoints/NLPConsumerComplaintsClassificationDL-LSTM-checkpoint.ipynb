{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4870212e-8584-458a-8b68-6b7567dffd9d",
   "metadata": {},
   "source": [
    "# Install requirements"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69b4a324",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install tensorflow_addons numpy pandas tensorflow sklearn nltk spacy textblob gensim scipy seaborn matplotlib minio mlflow wordcloud\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cea4e1d0-9de3-4fce-b43d-3a2463510d19",
   "metadata": {},
   "source": [
    "# Load Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9230902c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "import tensorflow_addons as tfa\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "import sklearn.feature_extraction.text as text\n",
    "from sklearn import model_selection, preprocessing, linear_model, naive_bayes, metrics, svm\n",
    "# from sklearn.naive_bayes import MultinomialNB\n",
    "# from sklearn.linear_model import LogisticRegression\n",
    "# from sklearn.ensemble import RandomForestClassifier\n",
    "# from sklearn.svm import LinearSVC\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "from textblob import TextBlob\n",
    "from nltk.stem import PorterStemmer,SnowballStemmer\n",
    "from textblob import Word\n",
    "from sklearn.feature_extraction.text import CountVectorizer,TfidfVectorizer\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "from nltk.tokenize.toktok import ToktokTokenizer\n",
    "# from wordcloud import WordCloudfrom wordcloud import WordCloud\n",
    "from io import StringIO\n",
    "import string\n",
    "import gensim\n",
    "from gensim.models import Word2Vec\n",
    "import itertools\n",
    "import scipy\n",
    "from scipy import spatial\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import re\n",
    "import nltk\n",
    "import joblib\n",
    "\n",
    "import mlflow\n",
    "import warnings\n",
    "\n",
    "from minio import Minio\n",
    "import subprocess\n",
    "import ipynbname\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "import absl.logging\n",
    "absl.logging.set_verbosity(absl.logging.ERROR)\n",
    "\n",
    "\n",
    "tokenizer = ToktokTokenizer()\n",
    "# stopword_list = nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "070e4ed1-1cad-4341-81d7-fa8780fc89b6",
   "metadata": {},
   "source": [
    "# Define the config file for mlflow and minio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e397a69",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf8ac49c-80bf-48ad-b041-a7eaf7dcc803",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "35839aee-02f3-41b0-9997-1af90b582a28",
   "metadata": {},
   "outputs": [],
   "source": [
    "HOST = \"http://mlflow:5500\"\n",
    "\n",
    "PROJECT_NAME = \"NlpTc\"\n",
    "EXPERIMENT_NAME = \"NlpLstm\"\n",
    "\n",
    "os.environ['MLFLOW_S3_ENDPOINT_URL']='http://minio-ml-workshop:9000'\n",
    "os.environ['AWS_ACCESS_KEY_ID']='minio'\n",
    "os.environ['AWS_SECRET_ACCESS_KEY']='minio123'\n",
    "os.environ['AWS_REGION']='us-east-1'\n",
    "os.environ['AWS_BUCKET_NAME']='raw-data-saeed'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e2201c73-0605-41b1-8f4b-924aae88fd44",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_s3_server():\n",
    "    minioClient = Minio('minio-ml-workshop:9000',\n",
    "                    access_key='minio',\n",
    "                    secret_key='minio123',\n",
    "                    secure=False)\n",
    "\n",
    "    return minioClient"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "bb3ae1c9-5dcb-4bca-bbe2-7d7e026b4ef0",
   "metadata": {},
   "outputs": [],
   "source": [
    "client = get_s3_server()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22deba8c-4667-4958-b114-93d5c3094fd9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "af49b7ed-6635-42d8-8981-05ec30a53d0a",
   "metadata": {},
   "source": [
    "# Load MLFlow to track the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d419a40c-2a99-4b35-aa6c-708f8414dad7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from verta.utils import ModelAPI\n",
    "\n",
    "# Connect to local MLflow tracking server\n",
    "mlflow.set_tracking_uri(HOST)\n",
    "\n",
    "# Set the experiment name...\n",
    "mlflow.set_experiment(EXPERIMENT_NAME)\n",
    "\n",
    "mlflow.tensorflow.autolog()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c860bf3b-002c-4a00-b789-41028de2098f",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def get_git_revision_hash():\n",
    "    return subprocess.check_output(['git', 'rev-parse', 'HEAD'])\n",
    "\n",
    "def get_git_revision_short_hash():\n",
    "    return subprocess.check_output(['git', 'rev-parse', '--short', 'HEAD'])\n",
    "\n",
    "def get_git_remote():\n",
    "    return subprocess.check_output(['git', 'config', '--get', 'remote.origin.url'])\n",
    "\n",
    "def get_git_user():\n",
    "    return subprocess.check_output(['git', 'config', 'user.name'])\n",
    "\n",
    "def get_git_branch():\n",
    "    return subprocess.check_output(['git', 'branch', '--show-current'])\n",
    "\n",
    "def get_pip_freeze():\n",
    "    return subprocess.check_output(['pip', 'freeze']).splitlines()\n",
    "\n",
    "\n",
    "def record_details(mlflow):\n",
    "    \"\"\"\n",
    "    This method is the anchor poijt and more activiteis will go in it\n",
    "    :param mlflow:\n",
    "    :return:\n",
    "    \"\"\"\n",
    "    with open(\"pip_freeze.txt\", \"wb\") as file:\n",
    "        for line in get_pip_freeze():\n",
    "            file.write(line)\n",
    "            file.write(bytes(\"\\n\", \"UTF-8\"))\n",
    "    mlflow.log_artifact(\"pip_freeze.txt\")\n",
    "    file.close()\n",
    "    mlflow.log_artifact(\"model.h5\", artifact_path=\"model\")\n",
    "    mlflow.log_artifact(\"tokenizer.pkl\", artifact_path=\"model\")\n",
    "    mlflow.log_artifact(\"labelencoder.pkl\", artifact_path=\"model\")\n",
    "    \n",
    "    os.remove(\"pip_freeze.txt\")\n",
    "    os.remove(\"model.h5\")\n",
    "    os.remove(\"tokenizer.pkl\")\n",
    "    os.remove(\"labelencoder.pkl\")\n",
    "\n",
    "\n",
    "\n",
    "def mlflow_grid_search(methodtoexecute, methodarguments):\n",
    "    with mlflow.start_run(tags= {\n",
    "        \"mlflow.source.git.commit\" : get_git_revision_hash() ,\n",
    "        \"mlflow.user\": get_git_user(),\n",
    "        \"mlflow.source.git.repoURL\": get_git_remote(),\n",
    "        \"git_remote\": get_git_remote(),\n",
    "        \"mlflow.source.git.branch\": get_git_branch(),\n",
    "        \"mlflow.docker.image.name\": os.getenv(\"JUPYTER_IMAGE\", \"LOCAL\"),\n",
    "        \"mlflow.source.type\": \"NOTEBOOK\",\n",
    "#         \"mlflow.source.name\": ipynbname.name()\n",
    "    }) as run:\n",
    "        methodtoexecute(**methodarguments)\n",
    "        record_details(mlflow)\n",
    "\n",
    "    return run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ff849801-acfa-4877-b518-edbac3972d48",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fetch_logged_data(run_id):\n",
    "    client = mlflow.tracking.MlflowClient()\n",
    "    data = client.get_run(run_id).data\n",
    "    tags = {k: v for k, v in data.tags.items() if not k.startswith(\"mlflow.\")}\n",
    "    artifacts = [f.path for f in client.list_artifacts(run_id, \"model\")]\n",
    "    return data.params, data.metrics, tags, artifacts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2523b21-5c01-448b-b06c-ea39181c408e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "0fb98f36",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /opt/app-\n",
      "[nltk_data]     root/src/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('stopwords')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "0046817d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to /opt/app-\n",
      "[nltk_data]     root/src/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('wordnet')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ce9de56",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "1aabb275",
   "metadata": {},
   "source": [
    "# Readinng the data\n",
    "Reading the data from S3 bucket\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f0edbdd4-2dd1-41d8-9a98-21fdd0179517",
   "metadata": {},
   "outputs": [],
   "source": [
    "csv_file = client.get_object(\"raw-data-saeed\", \"data.csv\")\n",
    "df1 = pd.read_csv(csv_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b8e937c",
   "metadata": {},
   "source": [
    "# Word Cloud for all Product categories\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf6e8e5c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "53c572f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# for product_name in df1['product'].unique():\n",
    "#     print(product_name)\n",
    "#     all_words = ' '.join([text for text in df1.loc[df1['product'].str.contains(product_name),'consumer_complaint_narrative']])\n",
    "    \n",
    "#     wordcloud = WordCloud(width=800, height=500, random_state=21, max_font_size=110).generate(all_words)\n",
    "\n",
    "#     plt.figure(figsize=(10, 7))\n",
    "#     plt.imshow(wordcloud, interpolation=\"bilinear\")\n",
    "#     plt.axis('off')\n",
    "#     plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "b390824e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2000, 13)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df1.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fad62b0d",
   "metadata": {},
   "source": [
    "### Train/Test split\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "adcce9c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_x, valid_x, train_y, valid_y = train_test_split(df1['consumer_complaint_narrative'], df1['product'],stratify=df1['product'], \n",
    "                                                    test_size=0.30)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "9f0e09e0-0f78-4287-a330-34167e1386a8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Consumer Loan'"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_x = train_x\n",
    "ind = 2\n",
    "train_x.iloc[ind]\n",
    "train_y.iloc[ind]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb2077aa",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Feature engineering of consumer complaint with TF-IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "8dea78e0",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Bank account or service' 'Consumer Loan' 'Credit card'\n",
      " 'Credit reporting' 'Debt collection' 'Money transfers' 'Mortgage'\n",
      " 'Other financial service' 'Payday loan' 'Prepaid card' 'Student loan']\n",
      "(array([ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10]), array([104,  79, 161, 212, 382,  19, 350,   2,  23,   8,  60]))\n",
      "(array([ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10]), array([ 45,  33,  69,  91, 163,   8, 150,   1,  10,   4,  26]))\n"
     ]
    }
   ],
   "source": [
    "\n",
    "##label encoding target variable\n",
    "enc = preprocessing.LabelEncoder()\n",
    "train_labels = enc.fit_transform(train_y)\n",
    "test_labels = enc.fit_transform(valid_y)\n",
    "\n",
    "print(enc.classes_)\n",
    "print(np.unique(train_labels, return_counts=True))\n",
    "print(np.unique(test_labels, return_counts=True))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "cf72886f",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "##tf-idf verctor representation\n",
    "tfidf_vect = TfidfVectorizer(analyzer='word', token_pattern=r'\\w{1,}', max_features=5000)\n",
    "tfidf_vect.fit(df1['consumer_complaint_narrative'])\n",
    "xtrain_tfidf =  tfidf_vect.transform(train_x)\n",
    "xvalid_tfidf =  tfidf_vect.transform(valid_x)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f577c518",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Deep Learning models\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "39f05618",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from tensorflow.keras.layers import Dense, Input, LSTM, Embedding, Dropout, Activation\n",
    "from tensorflow.keras.layers import Bidirectional, GlobalMaxPool1D, Conv1D, SimpleRNN\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras import initializers, regularizers, constraints, optimizers, layers\n",
    "from tensorflow.keras.layers import Dense, Input, Flatten, Dropout, BatchNormalization\n",
    "from tensorflow.keras.layers import Conv1D, MaxPooling1D, Embedding\n",
    "from tensorflow.keras.models import Sequential\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "6cf7ab91",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "total_complaints = np.append(train_x.values,valid_x.values)\n",
    "tokenizer = Tokenizer(num_words=2000)\n",
    "tokenizer.fit_on_texts(train_x.values)#total_complaints\n",
    "\n",
    "train_sequences = tokenizer.texts_to_sequences(train_x.values)\n",
    "test_sequences = tokenizer.texts_to_sequences(valid_x.values)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "e1bf6d0e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 7031 unique tokens.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "word_index = tokenizer.word_index# dictionary containing words and their index\n",
    "print('Found %s unique tokens.' % len(word_index))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "d9e2e84a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "348"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "MAX_SEQUENCE_LENGTH = max([len(c.split()) for c in total_complaints])\n",
    "MAX_SEQUENCE_LENGTH\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "9d594e51",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1400, 348)\n",
      "(600, 348)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "train_data = pad_sequences(train_sequences, maxlen=MAX_SEQUENCE_LENGTH,padding='post')\n",
    "test_data = pad_sequences(test_sequences, maxlen=MAX_SEQUENCE_LENGTH,padding='post')\n",
    "print(train_data.shape)\n",
    "print(test_data.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "346cd056-11fa-4922-a51d-c88c6778254f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['Bank account or service', 'Consumer Loan', 'Credit card',\n",
       "       'Credit reporting', 'Debt collection', 'Money transfers',\n",
       "       'Mortgage', 'Other financial service', 'Payday loan',\n",
       "       'Prepaid card', 'Student loan'], dtype=object)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "enc.classes_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "254cb16f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of data tensor: (1400, 348)\n",
      "Shape of label tensor: (1400, 11)\n",
      "Shape of label tensor: (600, 11)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "labels_train = to_categorical(np.asarray(train_labels))\n",
    "labels_test = to_categorical(np.asarray(test_labels))\n",
    "print('Shape of data tensor:', train_data.shape)\n",
    "print('Shape of label tensor:', labels_train.shape)\n",
    "print('Shape of label tensor:', labels_test.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4e7644c-1949-408f-a75b-12e66316b1b1",
   "metadata": {},
   "source": [
    "\n",
    "## CNN w/ Pre-trained word embeddings(GloVe)\n",
    "We’ll use pre-trained embeddings such as Glove which provides word based vector representation trained on a large corpus.\n",
    "\n",
    "It is trained on a dataset of one billion tokens (words) with a vocabulary of 400 thousand words. The glove has embedding vector sizes, including 50, 100, 200 and 300 dimensions.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "ca0113f9-7cf3-480e-be8e-5bc6f1a16230",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 400000 word vectors.\n"
     ]
    }
   ],
   "source": [
    "# !wget http://nlp.stanford.edu/data/glove.6B.zip\n",
    "f = client.get_object(\"raw-data-saeed\", \"glove.6B.50d.txt\")\n",
    "embeddings_index = {}\n",
    "# f = open(os.path.join(GLOVE_DIR, 'glove.6B.50d.txt'))\n",
    "# f = open( 'glove.6B.50d.txt')\n",
    "for line in f:\n",
    "    # print(line.decode(\"utf-8\") )\n",
    "    line = line.decode(\"utf-8\")\n",
    "    # break\n",
    "    values = line.split()\n",
    "    word = values[0]\n",
    "    coefs = np.asarray(values[1:], dtype='float32')\n",
    "    embeddings_index[word] = coefs\n",
    "f.close()\n",
    "\n",
    "print('Found %s word vectors.' % len(embeddings_index))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86779d7e",
   "metadata": {},
   "source": [
    "Now lets create the embedding matrix using the word indexer created from tokenizer.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "45826314",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "EMBEDDING_DIM = 50\n",
    "embedding_matrix = np.zeros((len(word_index) + 1, EMBEDDING_DIM))\n",
    "for word, i in word_index.items():\n",
    "    embedding_vector = embeddings_index.get(word)\n",
    "    if embedding_vector is not None:\n",
    "        # words not found in embedding index will be all-zeros.\n",
    "        embedding_matrix[i] = embedding_vector\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a42aea74",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Lets check the word embedded vector representation for token ‘loan’ in our embedding matrix\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "e34e9df8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('loan', 4)]"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "\n",
    "[(k,v) for k,v in word_index.items() if v==4]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "d1eea10d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.93484002,  0.40450999,  0.10856   , -0.61953998, -0.69220001,\n",
       "        0.32119   , -0.70885003,  0.071233  , -0.33484   ,  0.77158999,\n",
       "       -0.050077  ,  1.14460003,  0.01926   , -1.02590001,  0.85535002,\n",
       "       -0.081615  ,  0.19649   , -0.051262  ,  0.40103999,  0.87255001,\n",
       "        0.95371002, -0.87009001, -0.81568998, -0.24765   , -1.44400001,\n",
       "       -0.88612998,  1.51440001, -0.014284  , -0.48023999, -0.32289001,\n",
       "        3.00580001,  0.49408999,  0.72916001,  0.60891002,  0.59543997,\n",
       "        0.49731001, -0.0057787 , -0.21278   ,  0.94937998, -2.16849995,\n",
       "        0.12593   , -0.56818998,  0.50354999,  0.013716  , -1.01310003,\n",
       "       -0.46805999,  0.17305   ,  1.62039995,  0.60404998,  0.063104  ])"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embedding_matrix[4]  ## word embedded vector representation for token 'loan'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "a4e3e401",
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_size = len(tokenizer.word_index)+1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "3afcf31a",
   "metadata": {},
   "outputs": [],
   "source": [
    "## RNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "0f2f5ce7-e74f-49b4-af05-cbb0b9e2ee0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.layers import Bidirectional, Dense, Input, LSTM, Embedding\n",
    "from tensorflow.keras.models import Sequential\n",
    "\n",
    "\n",
    "\n",
    "class BuildModel():\n",
    "    '''\n",
    "    Build Lstm model for tensorflow\n",
    "    ----------\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    self.model:\n",
    "        Deep learning based Model\n",
    "    \n",
    "    '''\n",
    "    def __init__(self, WORD_INDEX,EMWEIGHTS, EMBEDDING_DIM= 50,MAX_SEQUENCE_LENGTH= 348, LOSS='categorical_crossentropy',OPTIMIZER='rmsprop',METRICS=['acc'],NUM_CLASSES=11,DROP_OUT_RATE =.4 ):\n",
    "        self.weights = [EMWEIGHTS]\n",
    "        self.input_length = MAX_SEQUENCE_LENGTH\n",
    "        self.embeding_dim = EMBEDDING_DIM\n",
    "        self.loss = LOSS\n",
    "        self.optimizer = OPTIMIZER\n",
    "        self.metrics = METRICS\n",
    "        self.model = []\n",
    "        self.num_classes = NUM_CLASSES\n",
    "        self.drate = DROP_OUT_RATE\n",
    "        self.word_index = WORD_INDEX\n",
    "        \n",
    "    def DefineModel(self):\n",
    "        '''\n",
    "        Define the model\n",
    "        ----------\n",
    "        \n",
    "        Returns\n",
    "        -------\n",
    "        \n",
    "        '''\n",
    "        #Bidirectional LSTM\n",
    "        self.model = Sequential()\n",
    "        self.model.add(Embedding(len(self.word_index) + 1,\n",
    "                                    self.embeding_dim,\n",
    "#                                     weights=self.weights,\n",
    "                                    input_length=self.input_length ,\n",
    "                                    trainable=True))\n",
    "        self.model.add(Bidirectional(LSTM(100, dropout = self.drate, return_sequences=True)))\n",
    "        self.model.add(Bidirectional(LSTM(256, dropout = self.drate)))\n",
    "        self.model.add(Dense(self.num_classes,activation='sigmoid'))\n",
    "        # return self.final_set,self.labels, self.enc, self.ohe,self.encoding_flag\n",
    "    def CompileModel(self):\n",
    "        '''\n",
    "        Compile the model\n",
    "        ----------\n",
    "        \n",
    "        Returns\n",
    "        -------\n",
    "        \n",
    "        '''\n",
    "        self.model.compile(loss=self.loss,\n",
    "              optimizer=self.optimizer,\n",
    "              metrics=self.metrics)\n",
    "#         return self.model\n",
    "    def BuildModel(self):\n",
    "        '''\n",
    "        Build the model\n",
    "        ----------\n",
    "        \n",
    "        Returns\n",
    "        -------\n",
    "        \n",
    "        '''\n",
    "        self.DefineModel()\n",
    "        self.CompileModel()\n",
    "        return self.model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "b740adae-5c4f-4d0c-90ef-b8cfb9da8720",
   "metadata": {},
   "outputs": [],
   "source": [
    "model2 = BuildModel(WORD_INDEX=word_index,EMWEIGHTS = embedding_matrix).BuildModel()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "8d516f5b-c95f-46f5-93c4-21f7b01e2d46",
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint_filepath = 'model.h5'\n",
    "model_checkpoint_callback = tf.keras.callbacks.ModelCheckpoint(\n",
    "    filepath=checkpoint_filepath,\n",
    "    monitor='val_acc',\n",
    "    mode='max',\n",
    "    save_best_only=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "46397240-0169-4333-8897-49a04e3447bc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "11/22 [==============>...............] - ETA: 15s - loss: 2.1504 - acc: 0.2699"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-40-67cb4af4917a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;31m#         \"mlflow.source.name\": ipynbname.name()\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m     }) as run:\n\u001b[0;32m---> 14\u001b[0;31m         model2.fit(train_data, labels_train,\n\u001b[0m\u001b[1;32m     15\u001b[0m                  \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m64\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m                  \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/app-root/lib/python3.8/site-packages/mlflow/utils/autologging_utils/safety.py\u001b[0m in \u001b[0;36msafe_patch_function\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    521\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    522\u001b[0m                     \u001b[0;32mif\u001b[0m \u001b[0mpatch_is_class\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 523\u001b[0;31m                         \u001b[0mpatch_function\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcall\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcall_original\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    524\u001b[0m                     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    525\u001b[0m                         \u001b[0mpatch_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcall_original\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/app-root/lib/python3.8/site-packages/mlflow/utils/autologging_utils/safety.py\u001b[0m in \u001b[0;36mcall\u001b[0;34m(cls, original, *args, **kwargs)\u001b[0m\n\u001b[1;32m    154\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0mclassmethod\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    155\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mcall\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcls\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moriginal\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 156\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mcls\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__call__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moriginal\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    157\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    158\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__call__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moriginal\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/app-root/lib/python3.8/site-packages/mlflow/utils/autologging_utils/safety.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, original, *args, **kwargs)\u001b[0m\n\u001b[1;32m    165\u001b[0m                 \u001b[0;31m# Regardless of what happens during the `_on_exception` callback, reraise\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    166\u001b[0m                 \u001b[0;31m# the original implementation exception once the callback completes\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 167\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    168\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    169\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/app-root/lib/python3.8/site-packages/mlflow/utils/autologging_utils/safety.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, original, *args, **kwargs)\u001b[0m\n\u001b[1;32m    158\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__call__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moriginal\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    159\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 160\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_patch_implementation\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moriginal\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    161\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mException\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mKeyboardInterrupt\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    162\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/app-root/lib/python3.8/site-packages/mlflow/utils/autologging_utils/safety.py\u001b[0m in \u001b[0;36m_patch_implementation\u001b[0;34m(self, original, *args, **kwargs)\u001b[0m\n\u001b[1;32m    216\u001b[0m                     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmanaged_run\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtry_mlflow_log\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcreate_managed_run\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    217\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 218\u001b[0;31m                 result = super(PatchWithManagedRun, self)._patch_implementation(\n\u001b[0m\u001b[1;32m    219\u001b[0m                     \u001b[0moriginal\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    220\u001b[0m                 )\n",
      "\u001b[0;32m/opt/app-root/lib/python3.8/site-packages/mlflow/tensorflow.py\u001b[0m in \u001b[0;36m_patch_implementation\u001b[0;34m(self, original, inst, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1151\u001b[0m                 \u001b[0m_log_early_stop_callback_params\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mearly_stop_callback\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1152\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1153\u001b[0;31m                 \u001b[0mhistory\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0moriginal\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minst\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1154\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1155\u001b[0m                 _log_early_stop_callback_metrics(\n",
      "\u001b[0;32m/opt/app-root/lib/python3.8/site-packages/mlflow/utils/autologging_utils/safety.py\u001b[0m in \u001b[0;36mcall_original\u001b[0;34m(*og_args, **og_kwargs)\u001b[0m\n\u001b[1;32m    479\u001b[0m                                 \u001b[0mdisable_warnings\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreroute_warnings\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    480\u001b[0m                             ):\n\u001b[0;32m--> 481\u001b[0;31m                                 \u001b[0moriginal_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0moriginal\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mog_args\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mog_kwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    482\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    483\u001b[0m                             try_log_autologging_event(\n",
      "\u001b[0;32m/opt/app-root/lib/python3.8/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[1;32m   1182\u001b[0m                 _r=1):\n\u001b[1;32m   1183\u001b[0m               \u001b[0mcallbacks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_train_batch_begin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1184\u001b[0;31m               \u001b[0mtmp_logs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1185\u001b[0m               \u001b[0;32mif\u001b[0m \u001b[0mdata_handler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshould_sync\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1186\u001b[0m                 \u001b[0mcontext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masync_wait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/app-root/lib/python3.8/site-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    883\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    884\u001b[0m       \u001b[0;32mwith\u001b[0m \u001b[0mOptionalXlaContext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jit_compile\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 885\u001b[0;31m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    886\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    887\u001b[0m       \u001b[0mnew_tracing_count\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexperimental_get_tracing_count\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/app-root/lib/python3.8/site-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    915\u001b[0m       \u001b[0;31m# In this case we have created variables on the first call, so we run the\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    916\u001b[0m       \u001b[0;31m# defunned version which is guaranteed to never create variables.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 917\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_stateless_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# pylint: disable=not-callable\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    918\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_stateful_fn\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    919\u001b[0m       \u001b[0;31m# Release the lock early so that multiple threads can perform the call\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/app-root/lib/python3.8/site-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   3037\u001b[0m       (graph_function,\n\u001b[1;32m   3038\u001b[0m        filtered_flat_args) = self._maybe_define_function(args, kwargs)\n\u001b[0;32m-> 3039\u001b[0;31m     return graph_function._call_flat(\n\u001b[0m\u001b[1;32m   3040\u001b[0m         filtered_flat_args, captured_inputs=graph_function.captured_inputs)  # pylint: disable=protected-access\n\u001b[1;32m   3041\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/app-root/lib/python3.8/site-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m_call_flat\u001b[0;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[1;32m   1961\u001b[0m         and executing_eagerly):\n\u001b[1;32m   1962\u001b[0m       \u001b[0;31m# No tape is watching; skip to running the function.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1963\u001b[0;31m       return self._build_call_outputs(self._inference_function.call(\n\u001b[0m\u001b[1;32m   1964\u001b[0m           ctx, args, cancellation_manager=cancellation_manager))\n\u001b[1;32m   1965\u001b[0m     forward_backward = self._select_forward_and_backward_functions(\n",
      "\u001b[0;32m/opt/app-root/lib/python3.8/site-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36mcall\u001b[0;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[1;32m    589\u001b[0m       \u001b[0;32mwith\u001b[0m \u001b[0m_InterpolateFunctionError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    590\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mcancellation_manager\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 591\u001b[0;31m           outputs = execute.execute(\n\u001b[0m\u001b[1;32m    592\u001b[0m               \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msignature\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    593\u001b[0m               \u001b[0mnum_outputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_num_outputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/app-root/lib/python3.8/site-packages/tensorflow/python/eager/execute.py\u001b[0m in \u001b[0;36mquick_execute\u001b[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[1;32m     57\u001b[0m   \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     58\u001b[0m     \u001b[0mctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mensure_initialized\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 59\u001b[0;31m     tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,\n\u001b[0m\u001b[1;32m     60\u001b[0m                                         inputs, attrs, num_outputs)\n\u001b[1;32m     61\u001b[0m   \u001b[0;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "joblib.dump(enc, 'labelencoder.pkl')  \n",
    "joblib.dump(tokenizer, 'tokenizer.pkl')  \n",
    "\n",
    "with mlflow.start_run(tags= {\n",
    "        \"mlflow.source.git.commit\" : get_git_revision_hash() ,\n",
    "        \"mlflow.user\": get_git_user(),\n",
    "        \"mlflow.source.git.repoURL\": get_git_remote(),\n",
    "        \"git_remote\": get_git_remote(),\n",
    "        \"mlflow.source.git.branch\": get_git_branch(),\n",
    "        \"mlflow.docker.image.name\": os.getenv(\"JUPYTER_IMAGE\", \"LOCAL\"),\n",
    "        \"mlflow.source.type\": \"NOTEBOOK\",\n",
    "#         \"mlflow.source.name\": ipynbname.name()\n",
    "    }) as run:\n",
    "        model2.fit(train_data, labels_train,\n",
    "                 batch_size=64,\n",
    "                 epochs=5,\n",
    "                 validation_data=(test_data, labels_test),callbacks=[model_checkpoint_callback])\n",
    "        record_details(mlflow)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "26114ec4-f87f-4706-97e5-7c6bf46cb37e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(600, 348)"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9f917fc-c479-4c3e-917b-3832e112b02f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install mlflow\n",
    "# !pip install minio\n",
    "# !pip install boto3\n",
    "# !pip install scikit-learn==0.24.2\n",
    "# !pip install openshift-client==1.0.13\n",
    "# !pip show mlflow\n",
    "# !pip show minio\n",
    "# !pip show boto3\n",
    "# !pip show scikit-learn\n",
    "# !pip show openshift-client\n",
    "\n",
    "import os\n",
    "import mlflow\n",
    "from minio import Minio\n",
    "import openshift as oc\n",
    "from jinja2 import Template\n",
    "\n",
    "os.environ['MLFLOW_S3_ENDPOINT_URL']='http://minio-ml-workshop:9000'\n",
    "os.environ['AWS_ACCESS_KEY_ID']='minio'\n",
    "os.environ['AWS_SECRET_ACCESS_KEY']='minio123'\n",
    "os.environ['AWS_REGION']='us-east-1'\n",
    "os.environ['AWS_BUCKET_NAME']='mlflow'\n",
    "# os.environ['MODEL_NAME'] = 'rossdemo'\n",
    "# os.environ['MODEL_VERSION'] = '1'\n",
    "# os.environ['OPENSHIFT_CLIENT_PYTHON_DEFAULT_OC_PATH'] = '/tmp/oc'\n",
    "\n",
    "HOST = \"http://mlflow:5500\"\n",
    "\n",
    "model_name = 'lstmv18'\n",
    "model_version = '1'\n",
    "build_name = f\"seldon-model-{model_name}-v{model_version}\"\n",
    "\n",
    "def get_s3_server():\n",
    "    minioClient = Minio('minio-ml-workshop:9000',\n",
    "                    access_key='minio',\n",
    "                    secret_key='minio123',\n",
    "                    secure=False)\n",
    "\n",
    "    return minioClient\n",
    "\n",
    "\n",
    "def init():\n",
    "    mlflow.set_tracking_uri(HOST)\n",
    "    print(HOST)\n",
    "    # Set the experiment name...\n",
    "    #mlflow_client = mlflow.tracking.MlflowClient(HOST)\n",
    "\n",
    "    \n",
    "def download_artifacts():\n",
    "    print(\"retrieving model metadata from mlflow...\")\n",
    "    model = mlflow.pyfunc.load_model(\n",
    "        model_uri=f\"models:/{model_name}/{model_version}\"\n",
    "    )\n",
    "    print(model)\n",
    "    \n",
    "    run_id = model.metadata.run_id\n",
    "    experiment_id = mlflow.get_run(run_id).info.experiment_id\n",
    "    \n",
    "    print(\"initializing connection to s3 server...\")\n",
    "    minioClient = get_s3_server()\n",
    "\n",
    "#     artifact_location = mlflow.get_experiment_by_name('rossdemo').artifact_location\n",
    "#     print(\"downloading artifacts from s3 bucket \" + artifact_location)\n",
    "\n",
    "    data_file_model = minioClient.fget_object(\"mlflow\", f\"/{experiment_id}/{run_id}/artifacts/model/model.h5\", \"model.h5\")\n",
    "    data_file_model2 = minioClient.fget_object(\"mlflow\", f\"/{experiment_id}/{run_id}/artifacts/model/tokenizer.pkl\", \"tokenizer.pkl\")\n",
    "    data_file_model3 = minioClient.fget_object(\"mlflow\", f\"/{experiment_id}/{run_id}/artifacts/model/labelencoder.pkl\", \"labelencoder.pkl\")\n",
    "\n",
    "\n",
    "    #Using boto3 Download the files from mlflow, the file path is in the model meta\n",
    "    #write the files to the file system\n",
    "    print(\"download successful\")\n",
    "    \n",
    "    return run_id\n",
    "    \n",
    "        \n",
    "init()\n",
    "run_id = download_artifacts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0528899f-653a-428b-804e-42d93bc9f9ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import joblib\n",
    "import numpy as np\n",
    "import json\n",
    "import traceback\n",
    "import sys\n",
    "class Predictor(object):\n",
    "\n",
    "    def __init__(self):\n",
    "        self.model = tf.keras.models.load_model('model.h5', compile=False)\n",
    "        self.labelencoder = joblib.load('labelencoder.pkl')\n",
    "\n",
    "\n",
    "\n",
    "    def predict(self, X,features_names):\n",
    "        # data = request.get(\"data\", {}).get(\"ndarray\")\n",
    "        # mult_types_array = np.array(data, dtype=object)\n",
    "        print ('step1......')\n",
    "        print(X)\n",
    "        X = tf.constant(X)\n",
    "        print ('step2......')\n",
    "        print(X)\n",
    "#         result = self.model.predict(X)\n",
    "        try:\n",
    "            result = self.model.predict(X)\n",
    "        except Exception as e:\n",
    "            print(traceback.format_exception(*sys.exc_info()))\n",
    "            raise # reraises the exception\n",
    "        \n",
    "\n",
    "                \n",
    "        print ('step3......')\n",
    "        result = tf.sigmoid(result)\n",
    "        print ('step4......')\n",
    "        print(result)\n",
    "        result = tf.math.argmax(result,axis=1)\n",
    "        print ('step5......')\n",
    "        print(result)\n",
    "        print(result.shape)\n",
    "        \n",
    "        print(self.labelencoder.inverse_transform(result))\n",
    "        print ('step6......')\n",
    "        return json.dumps(result, cls=JsonSerializer)\n",
    "\n",
    "class JsonSerializer(json.JSONEncoder):\n",
    "    def default(self, obj):\n",
    "        if isinstance(obj, (\n",
    "        np.int_, np.intc, np.intp, np.int8, np.int16, np.int32, np.int64, np.uint8, np.uint16, np.uint32, np.uint64)):\n",
    "            return int(obj)\n",
    "        elif isinstance(obj, (np.float_, np.float16, np.float32, np.float64)):\n",
    "            return float(obj)\n",
    "        elif isinstance(obj, (np.ndarray,)):\n",
    "            return obj.tolist()\n",
    "        return json.JSONEncoder.default(self, obj)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c432a41-0312-4d66-a211-790f376afa09",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import pandas as pd\n",
    "import joblib\n",
    "import numpy as np\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "import json\n",
    "\n",
    "class Transformer(object):\n",
    "    def __init__(self):\n",
    "        self.tokenizer = joblib.load('tokenizer.pkl')\n",
    "        \n",
    "    def transform_input(self, X, feature_names, meta):\n",
    "        # print(request)\n",
    "        X = X.get(\"data\", {}).get(\"ndarray\")\n",
    "        print(X)\n",
    "        output = self.tokenizer.texts_to_sequences(X)\n",
    "        print(X)\n",
    "        \n",
    "        print(output)\n",
    "        output = pad_sequences(output, maxlen=348,padding='post')\n",
    "        print(output)\n",
    "#         output = tf.constant(output)\n",
    "#         print(output)\n",
    "        return output\n",
    "\n",
    "\n",
    "# import tensorflow as tf\n",
    "# import pandas as pd\n",
    "# from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "# class Transformer(object):\n",
    "#     def __init__(self):\n",
    "#         self.ordinalencoder = joblib.load('ordinalencoder.pkl')\n",
    "        \n",
    "#     def transform_input(self, request):\n",
    "#         X = request.get(\"data\", {}).get(\"ndarray\")\n",
    "#         feature_names = request.get(\"data\", {}).get(\"names\")\n",
    "        \n",
    "#         df = pd.DataFrame(X, columns=feature_names)\n",
    "#         df = ordinalencoder.transform(df)\n",
    "#         df = onehotencoder.transform(df)\n",
    "#         train_sequences = tokenizer.texts_to_sequences(train_x.values)\n",
    "\n",
    "\n",
    "#         #df = df.drop(['customerID'], axis=1)\n",
    "#         return df.to_numpy()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9428b35",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig1 = plt.figure()\n",
    "plt.plot(history.history['loss'],'r',linewidth=3.0)\n",
    "plt.plot(history.history['val_loss'],'b',linewidth=3.0)\n",
    "plt.legend(['Training loss', 'Validation Loss'],fontsize=18)\n",
    "plt.xlabel('Epochs ',fontsize=16)\n",
    "plt.ylabel('Loss',fontsize=16)\n",
    "plt.title('Loss Curves :RNN - LSTM',fontsize=16)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d876903",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig1 = plt.figure()\n",
    "plt.plot(history.history['acc'],'r',linewidth=3.0)\n",
    "plt.plot(history.history['val_acc'],'b',linewidth=3.0)\n",
    "plt.legend(['Training acc', 'Validation acc'],fontsize=18)\n",
    "plt.xlabel('Epochs ',fontsize=16)\n",
    "plt.ylabel('Accuracy',fontsize=16)\n",
    "plt.title('Accuracy Curves :RNN - LSTM',fontsize=16)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c922df7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e61836da",
   "metadata": {},
   "outputs": [],
   "source": [
    "#predictions on test data\n",
    "predicted=model.predict(test_data)\n",
    "predicted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89568132",
   "metadata": {},
   "outputs": [],
   "source": [
    "#model evaluation\n",
    "import sklearn\n",
    "from sklearn.metrics import precision_recall_fscore_support as score\n",
    "precision, recall, fscore, support = score(labels_test, predicted.round())\n",
    "print('precision: \\n{}'.format(precision))\n",
    "print('recall: \\n{}'.format(recall))\n",
    "print('fscore: \\n{}'.format(fscore))\n",
    "print('support: \\n{}'.format(support))\n",
    "print(\"############################\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5c09ebb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc0558f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(classification_report(labels_test, predicted.round(),target_names=df1['product'].unique()))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a92b61f",
   "metadata": {},
   "source": [
    "After hours of training we get good results with LSTM(type of recurrent neural network) compared to CNN. From the learning curves it is clear the model needs to be tuned for overfitting by selecting hyperparameters such as no of epochs via early stopping and dropout for regularization.\n",
    "\n",
    "We could further improve our final result by ensembling our xgboost and Neural network models by using Logistic Regression as our base model.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a35fdb6-cc8c-42ca-a15d-6815de5f7538",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "10d220c55dad87bc296f91a276cffc08c658df4f112171a2982baf6251a25e4e"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
