{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4870212e-8584-458a-8b68-6b7567dffd9d",
   "metadata": {},
   "source": [
    "# Install requirements"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69b4a324",
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install -r requirements.txt\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8e8d1d55-f228-462c-ac0f-dafc2e03bfb3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/opt/app-root/src/NLP-Consumer-Complain-Classification'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "cwd = os.getcwd()\n",
    "cwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5fc4b5c7-a1fc-42e8-8c00-f2987f6803c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "# sys.path is a list of absolute path strings\n",
    "sys.path.append(cwd)\n",
    "from src.dataloading.read_dataset import ReadData\n",
    "from src.features.build_features import BuildFeatures\n",
    "from src.modules.build_model import BuildModel\n",
    "from src.modules.train_model import TrainModel\n",
    "from src.modules.predict_model import Predictor,Transformer\n",
    "\n",
    "# from src.modules.predict_model import BuildModel\n",
    "# from src.modules.train_model import BuildModel\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cea4e1d0-9de3-4fce-b43d-3a2463510d19",
   "metadata": {},
   "source": [
    "# Load Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9230902c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import sys\n",
    "# # sys.path is a list of absolute path strings\n",
    "# sys.path.append('/opt/app-root/src/anz_ml_project/')\n",
    "\n",
    "# from src.features.build_features import BuildFeatures\n",
    "\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "import tensorflow_addons as tfa\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "import sklearn.feature_extraction.text as text\n",
    "from sklearn import model_selection, preprocessing, linear_model, naive_bayes, metrics, svm\n",
    "# from sklearn.naive_bayes import MultinomialNB\n",
    "# from sklearn.linear_model import LogisticRegression\n",
    "# from sklearn.ensemble import RandomForestClassifier\n",
    "# from sklearn.svm import LinearSVC\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "from textblob import TextBlob\n",
    "from nltk.stem import PorterStemmer,SnowballStemmer\n",
    "from textblob import Word\n",
    "from sklearn.feature_extraction.text import CountVectorizer,TfidfVectorizer\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "from nltk.tokenize.toktok import ToktokTokenizer\n",
    "# from wordcloud import WordCloudfrom wordcloud import WordCloud\n",
    "from io import StringIO\n",
    "import string\n",
    "import gensim\n",
    "from gensim.models import Word2Vec\n",
    "import itertools\n",
    "import scipy\n",
    "from scipy import spatial\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import re\n",
    "import nltk\n",
    "import joblib\n",
    "\n",
    "import warnings\n",
    "\n",
    "import subprocess\n",
    "import ipynbname\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "import absl.logging\n",
    "absl.logging.set_verbosity(absl.logging.ERROR)\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "\n",
    "\n",
    "tokenizer = ToktokTokenizer()\n",
    "# stopword_list = nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1aabb275",
   "metadata": {},
   "source": [
    "# Readinng the data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1c8f24db-e2bb-4dff-ba49-aa89fda5098c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Bank account or service' 'Consumer Loan' 'Credit card'\n",
      " 'Credit reporting' 'Debt collection' 'Money transfers' 'Mortgage'\n",
      " 'Other financial service' 'Payday loan' 'Prepaid card' 'Student loan']\n",
      "(array([ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10]), array([119,  90, 184, 242, 436,  22, 400,   2,  26,  10,  69]))\n",
      "(array([ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10]), array([ 30,  22,  46,  61, 109,   5, 100,   1,   7,   2,  17]))\n"
     ]
    }
   ],
   "source": [
    "train_data, test_data, train_labels, test_labels,enc = ReadData(DATA_PATH=\"./dataset/csv/data.csv\").ReadDataFrameData()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "50f7d20f-dba4-4216-8eff-7b3a5cfc12ef",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "pandas.core.frame.DataFrame"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_data1 = test_data.copy()\n",
    "type(test_data1)\n",
    "test_data2 = test_data1.to_frame()\n",
    "test_labels1 = test_labels\n",
    "type(test_data2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7541b519-9301-4792-8994-5a6a7ef5bb61",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "consumer_complaint_narrative    hello mortgage dispute report contracted exper...\n",
       "Name: 246, dtype: object"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "str1 = test_data2.iloc[1]\n",
    "str1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9b198734-8199-4497-b98a-e8c53f768eef",
   "metadata": {},
   "outputs": [],
   "source": [
    "label1= test_labels1[1]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d70fe7e6-69e8-4ba7-bfcf-16a5fcf2f0e4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step1: Loading models\n",
      "Loading model 2677\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-02-25 04:31:03.331094: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcuda.so.1'; dlerror: libcuda.so.1: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/local/nvidia/lib:/usr/local/nvidia/lib64\n",
      "2022-02-25 04:31:03.331127: W tensorflow/stream_executor/cuda/cuda_driver.cc:269] failed call to cuInit: UNKNOWN ERROR (303)\n",
      "2022-02-25 04:31:03.331148: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:156] kernel driver does not appear to be running on this host (jupyterhub-nb-amirgholipour): /proc/driver/nvidia/version does not exist\n",
      "2022-02-25 04:31:03.331356: I tensorflow/core/platform/cpu_feature_guard.cc:151] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 AVX512F FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded model\n",
      "Step1 finished!!!!\n",
      "['Bank account or service' 'Consumer Loan' 'Credit card'\n",
      " 'Credit reporting' 'Debt collection' 'Money transfers' 'Mortgage'\n",
      " 'Other financial service' 'Payday loan' 'Prepaid card' 'Student loan']\n",
      "Step2: tokenise the input data.\n",
      "Step2 finished!!!!\n",
      "[[1525, 8, 83, 9, 55, 184, 83, 557, 3, 1, 9, 205, 8, 11, 257, 50, 19, 526, 1, 104, 25, 8, 11, 101, 2206, 956, 190, 225, 22, 3299, 180, 44, 285, 557, 1, 9, 245, 184, 124, 83, 263, 813, 8, 11, 51, 625, 107, 50, 3, 243, 1174, 50, 19, 430, 170, 1, 9, 1049, 449, 253, 2806, 966, 694, 204, 50, 19, 564, 1049, 193, 255, 1, 756, 180, 55, 8, 11, 228, 205, 285, 186, 143, 79, 802, 341, 7, 1222, 2207, 79, 140, 1, 9, 26, 341]]\n",
      "Step3: Do zero padding on the tokeize data.\n",
      "[[1525    8   83    9   55  184   83  557    3    1    9  205    8   11\n",
      "   257   50   19  526    1  104   25    8   11  101 2206  956  190  225\n",
      "    22 3299  180   44  285  557    1    9  245  184  124   83  263  813\n",
      "     8   11   51  625  107   50    3  243 1174   50   19  430  170    1\n",
      "     9 1049  449  253 2806  966  694  204   50   19  564 1049  193  255\n",
      "     1  756  180   55    8   11  228  205  285  186  143   79  802  341\n",
      "     7 1222 2207   79  140    1    9   26  341    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0]]\n",
      "Step3 finished!!!!\n",
      "Step4:  Do prediction!!!\n",
      "Step4 finished!!!!\n",
      "(1, 11)\n",
      "Predicted Class name:  6\n",
      "Predicted class Certainty:  0.8476088\n",
      "['Mortgage']\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'Predicted Class': 6,\n",
       " 'Predicted Class Label': array(['Mortgage'], dtype=object),\n",
       " 'Predicted Certainty Score': '0.8476088'}"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from prediction import predict\n",
    "\n",
    "predict( str1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "2c06c482-66d6-4ee3-9741-461ce26560d0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['./models/labelencoder.pkl']"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "joblib.dump(enc, './models/labelencoder.pkl')  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "409a6120-e387-4d17-aa38-7768f7c279b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from src.features.build_features import BuildFeatures\n",
    "train_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05cf964d-4da8-424e-ab28-d6fd39e6a3c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_labels.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4f116d8-378f-42ba-ae65-f6bda55a4e63",
   "metadata": {},
   "source": [
    "# Prepare data for modeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c80efa33-a23a-427e-8711-9b661198c995",
   "metadata": {},
   "outputs": [],
   "source": [
    "#### if you would like to use glove weights\n",
    "weight_flag = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e347a28f-63d6-4aa1-aa8e-7f24b2ce0650",
   "metadata": {},
   "outputs": [],
   "source": [
    "BFCLASS = BuildFeatures(TRAIN_DATA=train_data,TEST_DATA=test_data,TRAIN_LABELS=train_labels,TEST_LABELS=test_labels, GloveData=\"./dataset/glove.6B.50d.txt\",EMBEDDING_DIM=50, WEIGHT_FLAG = weight_flag)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c1e5e30-daed-4acd-8da9-028c765b79ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "if weight_flag== True:\n",
    "    train_data,test_data,train_labels , test_labels,word_index,tokenizer,MAX_SEQUENCE_LENGTH,embedding_matrix = BFCLASS.PreProcessingTextData()\n",
    "\n",
    "\n",
    "else:\n",
    "    train_data,test_data,train_labels , test_labels,word_index,tokenizer,MAX_SEQUENCE_LENGTH = BFCLASS.PreProcessingTextData()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22b3687a-45fe-48de-bf47-59e1a7f9e9c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "joblib.dump(tokenizer, './models/tokenizer.pkl')  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ae61e24-45bc-4ac9-9413-6995ce125fb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(word_index)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f577c518",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# Deep Learning define, train and test model\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "752ffe69-6213-4614-aa12-32b1cbdfd885",
   "metadata": {},
   "source": [
    "## Define the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b740adae-5c4f-4d0c-90ef-b8cfb9da8720",
   "metadata": {},
   "outputs": [],
   "source": [
    "if weight_flag== True:\n",
    "    model = BuildModel(WORD_INDEX=word_index, MAX_SEQUENCE_LENGTH=MAX_SEQUENCE_LENGTH, EMWEIGHTS=embedding_matrix).SetupModel()\n",
    "else:\n",
    "    model = BuildModel(WORD_INDEX=word_index, MAX_SEQUENCE_LENGTH=MAX_SEQUENCE_LENGTH, EMWEIGHTS=[]).SetupModel()    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c37d919-de54-4565-a4b6-74a58e4f1329",
   "metadata": {},
   "source": [
    "## Train the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df9aca2b-be70-4ca2-b89d-c33df03f2d3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "model, history= TrainModel(model, tokenizer, enc,train_data, train_labels,test_data, test_labels, BATCH_SIZE=64,EPOCHS=50).ModelTraining()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08d0644c-f188-43c9-a17c-c327145a641e",
   "metadata": {},
   "source": [
    "### Plot the training and testing Loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9428b35",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig1 = plt.figure()\n",
    "plt.plot(history.history['loss'],'r',linewidth=3.0)\n",
    "plt.plot(history.history['val_loss'],'b',linewidth=3.0)\n",
    "plt.legend(['Training loss', 'Validation Loss'],fontsize=18)\n",
    "plt.xlabel('Epochs ',fontsize=16)\n",
    "plt.ylabel('Loss',fontsize=16)\n",
    "plt.title('Loss Curves :RNN - LSTM',fontsize=16)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f7712ce-2e36-476e-80fa-68b568cb52f5",
   "metadata": {},
   "source": [
    "#### Plot the training and testing Accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d876903",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig1 = plt.figure()\n",
    "plt.plot(history.history['acc'],'r',linewidth=3.0)\n",
    "plt.plot(history.history['val_acc'],'b',linewidth=3.0)\n",
    "plt.legend(['Training acc', 'Validation acc'],fontsize=18)\n",
    "plt.xlabel('Epochs ',fontsize=16)\n",
    "plt.ylabel('Accuracy',fontsize=16)\n",
    "plt.title('Accuracy Curves :RNN - LSTM',fontsize=16)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a92b61f",
   "metadata": {},
   "source": [
    "After hours of training we get good results with LSTM(type of recurrent neural network) compared to CNN. From the learning curves it is clear the model needs to be tuned for overfitting by selecting hyperparameters such as no of epochs via early stopping and dropout for regularization.\n",
    "\n",
    "We could further improve our final result by ensembling our xgboost and Neural network models by using Logistic Regression as our base model.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80a43af4-e74f-43f9-a69d-c04475179984",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "10d220c55dad87bc296f91a276cffc08c658df4f112171a2982baf6251a25e4e"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
